{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2880231c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EXEMPLO DE PRIORIZAÇÃO DE BACKLOG ===\n",
      "\n",
      "1. Lendo e padronizando dados...\n",
      "   Carregados 28 itens\n",
      "2. Construindo grafo de dependências...\n",
      "Ciclo detectado. Removida aresta: ('21', '22')\n",
      "   Grafo: 28 nós, 31 arestas\n",
      "3. Calculando features do grafo...\n",
      "4. Treinando modelo de ML...\n",
      "   MAE CV: 0.8571\n",
      "5. Executando ordenação topológica gulosa...\n",
      "6. Calculando métricas finais...\n",
      "Resultados salvos em: /workspaces/codespaces-jupyter/data/backlog_priorizado.csv\n",
      "\n",
      "=== TOP 5 ITENS PRIORIZADOS ===\n",
      "   rank  id  roi_ajustado  score_ml  deps_ok  story_points\n",
      "0     1  22             3      2.25     True             3\n",
      "1     2  23             3      2.25    False             5\n",
      "2     3   1             1      2.25     True             1\n",
      "3     4   3             1      2.25    False             1\n",
      "4     5  13             3      2.25     True             3\n",
      "\n",
      "=== MÉTRICAS FINAIS ===\n",
      "MAE Cross-Validation: 0.8571\n",
      "Inversões Top-20: 0\n",
      "% Top-10 sem bloqueio: 30.0%\n",
      "N° de estimadores usados: 100\n",
      "\n",
      "=== IMPORTÂNCIA DAS FEATURES ===\n",
      "story_points: 0.0000\n",
      "in_degree: 0.0000\n",
      "out_degree: 0.0000\n",
      "dep_impact: 0.0000\n",
      "degree_centrality: 0.0000\n",
      "\n",
      "✅ Execução completa! Resultados salvos em 'exemplo_backlog_priorizado.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Tuple, Set, Optional, Any\n",
    "import networkx as nx\n",
    "from collections import defaultdict, deque\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class BacklogPrioritizer:\n",
    "    def __init__(self, \n",
    "                 decay_factor: float = 0.6,\n",
    "                 cv_folds: int = 5,\n",
    "                 random_state: int = 42):\n",
    "        \"\"\"\n",
    "        Algoritmo de priorização de backlog com ML e análise de grafos\n",
    "        \n",
    "        Args:\n",
    "            decay_factor: Fator de decaimento para impacto transitivo de dependências\n",
    "            cv_folds: Número de folds para cross-validation\n",
    "            random_state: Seed para reprodutibilidade\n",
    "        \"\"\"\n",
    "        self.decay_factor = decay_factor\n",
    "        self.cv_folds = cv_folds\n",
    "        self.random_state = random_state\n",
    "        self.model = None\n",
    "        self.label_encoders = {}\n",
    "        self.feature_importance = None\n",
    "        \n",
    "    def read_and_standardize(self, file_path: str) -> pd.DataFrame:\n",
    "        \"\"\"Lê e padroniza o CSV de entrada\"\"\"\n",
    "        # Detecção robusta de separador e encoding\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, encoding='utf-8')\n",
    "        except UnicodeDecodeError:\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, encoding='latin1')\n",
    "            except:\n",
    "                df = pd.read_csv(file_path, encoding='cp1252')\n",
    "        \n",
    "        # Se não funcionou com vírgula, tenta outros separadores\n",
    "        if len(df.columns) == 1:\n",
    "            for sep in [';', '\\t']:\n",
    "                try:\n",
    "                    df = pd.read_csv(file_path, sep=sep, encoding='utf-8')\n",
    "                    if len(df.columns) > 1:\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        # Padronizar nomes das colunas\n",
    "        column_mapping = {\n",
    "            'issueid': 'id',\n",
    "            'prioridade score': 'roi_ajustado',\n",
    "            'prioridade_score': 'roi_ajustado',\n",
    "            'storypoints': 'story_points',\n",
    "            'story_points': 'story_points',\n",
    "            'dependencias': 'deps',\n",
    "            'dependencies': 'deps'\n",
    "        }\n",
    "        \n",
    "        df.columns = df.columns.str.strip().str.lower()\n",
    "        df = df.rename(columns=column_mapping)\n",
    "        \n",
    "        # Validar colunas obrigatórias\n",
    "        required_cols = ['id', 'roi_ajustado', 'story_points', 'deps']\n",
    "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Colunas obrigatórias ausentes: {missing_cols}\")\n",
    "        \n",
    "        # Converter tipos\n",
    "        df['roi_ajustado'] = pd.to_numeric(df['roi_ajustado'], errors='coerce').fillna(0.0)\n",
    "        df['story_points'] = pd.to_numeric(df['story_points'], errors='coerce')\n",
    "        \n",
    "        # Tratar nulos em story_points\n",
    "        if df['story_points'].isna().all():\n",
    "            df['story_points'] = 3.0\n",
    "        else:\n",
    "            df['story_points'].fillna(df['story_points'].median(), inplace=True)\n",
    "        \n",
    "        # Parsear dependências\n",
    "        df['deps'] = df['deps'].apply(self._parse_dependencies)\n",
    "        \n",
    "        # Remover auto-dependências\n",
    "        df['deps'] = df.apply(lambda row: [dep for dep in row['deps'] if dep != row['id']], axis=1)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _parse_dependencies(self, deps_str: Any) -> List[str]:\n",
    "        \"\"\"Parseia string de dependências em lista\"\"\"\n",
    "        if pd.isna(deps_str) or deps_str == '':\n",
    "            return []\n",
    "        \n",
    "        deps_str = str(deps_str).strip()\n",
    "        if not deps_str:\n",
    "            return []\n",
    "        \n",
    "        # Tentar parsear como JSON\n",
    "        try:\n",
    "            if deps_str.startswith('[') and deps_str.endswith(']'):\n",
    "                return json.loads(deps_str)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Parsear com separadores\n",
    "        deps = []\n",
    "        for sep in [',', ';']:\n",
    "            if sep in deps_str:\n",
    "                deps = [dep.strip().strip('\"\\'') for dep in deps_str.split(sep)]\n",
    "                break\n",
    "        else:\n",
    "            deps = [deps_str.strip().strip('\"\\'')]\n",
    "        \n",
    "        return [dep for dep in deps if dep]\n",
    "    \n",
    "    def build_dependency_graph(self, df: pd.DataFrame) -> Tuple[nx.DiGraph, Dict]:\n",
    "        \"\"\"Constrói grafo de dependências\"\"\"\n",
    "        G = nx.DiGraph()\n",
    "        external_deps = {}\n",
    "        \n",
    "        # Adicionar todos os nós\n",
    "        valid_ids = set(df['id'].astype(str))\n",
    "        G.add_nodes_from(valid_ids)\n",
    "        \n",
    "        # Adicionar arestas\n",
    "        for _, row in df.iterrows():\n",
    "            item_id = str(row['id'])\n",
    "            external_deps[item_id] = []\n",
    "            \n",
    "            for dep in row['deps']:\n",
    "                dep = str(dep)\n",
    "                if dep in valid_ids:\n",
    "                    # Aresta: dependência → item (pré-requisito destrava item)\n",
    "                    G.add_edge(dep, item_id)\n",
    "                else:\n",
    "                    external_deps[item_id].append(dep)\n",
    "        \n",
    "        # Detectar e quebrar ciclos\n",
    "        if not nx.is_directed_acyclic_graph(G):\n",
    "            self._break_cycles(G, df)\n",
    "        \n",
    "        return G, external_deps\n",
    "    \n",
    "    def _break_cycles(self, G: nx.DiGraph, df: pd.DataFrame) -> None:\n",
    "        \"\"\"Quebra ciclos removendo arestas com menor prioridade\"\"\"\n",
    "        roi_dict = dict(zip(df['id'].astype(str), df['roi_ajustado']))\n",
    "        \n",
    "        while not nx.is_directed_acyclic_graph(G):\n",
    "            try:\n",
    "                cycle = nx.find_cycle(G)\n",
    "                # Encontrar aresta com menor prioridade no ciclo\n",
    "                min_weight = float('inf')\n",
    "                edge_to_remove = None\n",
    "                \n",
    "                for u, v in cycle:\n",
    "                    weight = roi_dict.get(u, 0) + roi_dict.get(v, 0)\n",
    "                    if weight < min_weight:\n",
    "                        min_weight = weight\n",
    "                        edge_to_remove = (u, v)\n",
    "                \n",
    "                if edge_to_remove:\n",
    "                    G.remove_edge(*edge_to_remove)\n",
    "                    print(f\"Ciclo detectado. Removida aresta: {edge_to_remove}\")\n",
    "                else:\n",
    "                    break\n",
    "            except nx.NetworkXNoCycle:\n",
    "                break\n",
    "    \n",
    "    def calculate_graph_features(self, df: pd.DataFrame, G: nx.DiGraph) -> pd.DataFrame:\n",
    "        \"\"\"Calcula features do grafo\"\"\"\n",
    "        result_df = df.copy()\n",
    "        roi_dict = dict(zip(df['id'].astype(str), df['roi_ajustado']))\n",
    "        \n",
    "        # Inicializar colunas\n",
    "        result_df['in_degree'] = 0\n",
    "        result_df['out_degree'] = 0\n",
    "        result_df['dep_impact'] = 0.0\n",
    "        result_df['degree_centrality'] = 0.0\n",
    "        result_df['deps_ok'] = True\n",
    "        \n",
    "        # Calcular métricas\n",
    "        centrality = nx.degree_centrality(G.to_undirected()) if len(G) > 1 else {}\n",
    "        \n",
    "        for idx, row in result_df.iterrows():\n",
    "            item_id = str(row['id'])\n",
    "            \n",
    "            # Graus\n",
    "            in_deg = G.in_degree(item_id)\n",
    "            out_deg = G.out_degree(item_id)\n",
    "            \n",
    "            result_df.loc[idx, 'in_degree'] = in_deg\n",
    "            result_df.loc[idx, 'out_degree'] = out_deg\n",
    "            result_df.loc[idx, 'deps_ok'] = (in_deg == 0)\n",
    "            result_df.loc[idx, 'degree_centrality'] = centrality.get(item_id, 0.0)\n",
    "            \n",
    "            # Impacto de dependências (soma do ROI dos dependentes diretos)\n",
    "            dependents = list(G.successors(item_id))\n",
    "            dep_impact = sum(roi_dict.get(dep, 0) for dep in dependents)\n",
    "            result_df.loc[idx, 'dep_impact'] = dep_impact\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    def train_ml_model(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Treina modelo de ML para priorização\"\"\"\n",
    "        # Selecionar features\n",
    "        base_features = ['story_points', 'in_degree', 'out_degree', 'dep_impact', 'degree_centrality']\n",
    "        categorical_features = []\n",
    "        \n",
    "        # Adicionar features categóricas se existirem\n",
    "        for col in ['team', 'component', 'type']:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].fillna('unknown')\n",
    "                le = LabelEncoder()\n",
    "                df[f'{col}_encoded'] = le.fit_transform(df[col])\n",
    "                base_features.append(f'{col}_encoded')\n",
    "                categorical_features.append(f'{col}_encoded')\n",
    "                self.label_encoders[col] = le\n",
    "        \n",
    "        X = df[base_features]\n",
    "        y = df['roi_ajustado']\n",
    "        \n",
    "        # Para datasets pequenos, usar abordagem mais simples\n",
    "        if len(df) < 50:\n",
    "            # Modelo simples sem CV extensivo\n",
    "            self.model = lgb.LGBMRegressor(\n",
    "                objective='regression',\n",
    "                n_estimators=100,\n",
    "                random_state=self.random_state,\n",
    "                verbose=-1\n",
    "            )\n",
    "            \n",
    "            self.model.fit(X, y)\n",
    "            mae_cv = mean_absolute_error(y, self.model.predict(X))\n",
    "            optimal_n_estimators = 100\n",
    "            \n",
    "        else:\n",
    "            # Cross-validation para datasets maiores\n",
    "            try:\n",
    "                # Criar bins para estratificação baseada no target\n",
    "                if len(y.unique()) < self.cv_folds:\n",
    "                    from sklearn.model_selection import KFold\n",
    "                    cv_splitter = KFold(n_splits=min(3, len(y)), shuffle=True, random_state=self.random_state)\n",
    "                    split_iterator = cv_splitter.split(X)\n",
    "                else:\n",
    "                    y_bins = pd.qcut(y, q=min(self.cv_folds, len(y.unique())), labels=False, duplicates='drop')\n",
    "                    cv_splitter = StratifiedKFold(n_splits=self.cv_folds, shuffle=True, random_state=self.random_state)\n",
    "                    split_iterator = cv_splitter.split(X, y_bins)\n",
    "                \n",
    "                n_estimators_list = []\n",
    "                mae_scores = []\n",
    "                \n",
    "                for train_idx, val_idx in split_iterator:\n",
    "                    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "                    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "                    \n",
    "                    model = lgb.LGBMRegressor(\n",
    "                        objective='regression',\n",
    "                        random_state=self.random_state,\n",
    "                        verbose=-1,\n",
    "                        n_estimators=200\n",
    "                    )\n",
    "                    \n",
    "                    # Treinar sem early stopping para evitar problemas\n",
    "                    model.fit(X_train, y_train)\n",
    "                    \n",
    "                    n_estimators_list.append(model.n_estimators)\n",
    "                    mae_scores.append(mean_absolute_error(y_val, model.predict(X_val)))\n",
    "                \n",
    "                # Modelo final\n",
    "                optimal_n_estimators = max(100, int(np.mean(n_estimators_list)) if n_estimators_list else 200)\n",
    "                mae_cv = np.mean(mae_scores) if mae_scores else 0.0\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Erro no CV, usando modelo simples: {e}\")\n",
    "                optimal_n_estimators = 200\n",
    "                mae_cv = 0.0\n",
    "            \n",
    "            # Treinar modelo final\n",
    "            self.model = lgb.LGBMRegressor(\n",
    "                objective='regression',\n",
    "                n_estimators=optimal_n_estimators,\n",
    "                random_state=self.random_state,\n",
    "                verbose=-1\n",
    "            )\n",
    "            \n",
    "            self.model.fit(X, y)\n",
    "        \n",
    "        # Feature importance\n",
    "        try:\n",
    "            self.feature_importance = dict(zip(base_features, self.model.feature_importances_))\n",
    "        except:\n",
    "            self.feature_importance = {feat: 1.0/len(base_features) for feat in base_features}\n",
    "        \n",
    "        # Predições\n",
    "        df['score_ml'] = self.model.predict(X)\n",
    "        \n",
    "        return {\n",
    "            'mae_cv': mae_cv,\n",
    "            'n_estimators': optimal_n_estimators,\n",
    "            'features': base_features\n",
    "        }\n",
    "    \n",
    "    def topological_greedy_sort(self, df: pd.DataFrame, G: nx.DiGraph) -> pd.DataFrame:\n",
    "        \"\"\"Ordenação topológica gulosa baseada em score_ml\"\"\"\n",
    "        result_df = df.copy()\n",
    "        G_work = G.copy()\n",
    "        \n",
    "        ranked_items = []\n",
    "        \n",
    "        while G_work.nodes():\n",
    "            # Encontrar nós sem dependências (in_degree == 0)\n",
    "            available = [node for node in G_work.nodes() if G_work.in_degree(node) == 0]\n",
    "            \n",
    "            if not available:\n",
    "                # Se há ciclos remanescentes, pegar qualquer nó\n",
    "                available = list(G_work.nodes())\n",
    "                print(\"Aviso: Ciclo detectado durante ordenação. Forçando progressão.\")\n",
    "            \n",
    "            # Criar mapping de score_ml para os disponíveis\n",
    "            available_scores = {}\n",
    "            for node in available:\n",
    "                node_data = result_df[result_df['id'].astype(str) == node].iloc[0]\n",
    "                available_scores[node] = (\n",
    "                    node_data['score_ml'],\n",
    "                    node_data['dep_impact'],\n",
    "                    -node_data['story_points'],  # Negativo para order crescente\n",
    "                    node\n",
    "                )\n",
    "            \n",
    "            # Escolher o melhor item (maior score_ml, depois critérios de desempate)\n",
    "            best_item = max(available_scores.items(), key=lambda x: x[1])[0]\n",
    "            \n",
    "            ranked_items.append(best_item)\n",
    "            \n",
    "            # Remover item escolhido do grafo\n",
    "            G_work.remove_node(best_item)\n",
    "        \n",
    "        # Atribuir ranks\n",
    "        rank_mapping = {item: rank + 1 for rank, item in enumerate(ranked_items)}\n",
    "        result_df['rank'] = result_df['id'].astype(str).map(rank_mapping)\n",
    "        \n",
    "        # Ordenar por rank\n",
    "        result_df = result_df.sort_values('rank').reset_index(drop=True)\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    def calculate_metrics(self, df: pd.DataFrame, G: nx.DiGraph) -> Dict:\n",
    "        \"\"\"Calcula métricas de qualidade da priorização\"\"\"\n",
    "        metrics = {}\n",
    "        \n",
    "        # Top-20 dependency inversions\n",
    "        top_20 = df.head(20)\n",
    "        inversions = 0\n",
    "        \n",
    "        for _, row in top_20.iterrows():\n",
    "            item_id = str(row['id'])\n",
    "            item_rank = row['rank']\n",
    "            \n",
    "            # Verificar se alguma dependência tem rank maior\n",
    "            for pred in G.predecessors(item_id):\n",
    "                pred_rank = df[df['id'].astype(str) == pred]['rank'].iloc[0]\n",
    "                if pred_rank > item_rank:\n",
    "                    inversions += 1\n",
    "        \n",
    "        metrics['inversions_top_20'] = inversions\n",
    "        \n",
    "        # % do top-10 sem bloqueio\n",
    "        top_10 = df.head(10)\n",
    "        unblocked = top_10['deps_ok'].sum()\n",
    "        metrics['pct_top10_unblocked'] = (unblocked / 10) * 100\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def prioritize_backlog(self, file_path: str) -> Tuple[pd.DataFrame, Dict]:\n",
    "        \"\"\"Executa todo o pipeline de priorização\"\"\"\n",
    "        print(\"1. Lendo e padronizando dados...\")\n",
    "        df = self.read_and_standardize(file_path)\n",
    "        print(f\"   Carregados {len(df)} itens\")\n",
    "        \n",
    "        print(\"2. Construindo grafo de dependências...\")\n",
    "        G, external_deps = self.build_dependency_graph(df)\n",
    "        print(f\"   Grafo: {len(G.nodes())} nós, {len(G.edges())} arestas\")\n",
    "        \n",
    "        print(\"3. Calculando features do grafo...\")\n",
    "        df = self.calculate_graph_features(df, G)\n",
    "        \n",
    "        print(\"4. Treinando modelo de ML...\")\n",
    "        ml_metrics = self.train_ml_model(df)\n",
    "        print(f\"   MAE CV: {ml_metrics['mae_cv']:.4f}\")\n",
    "        \n",
    "        print(\"5. Executando ordenação topológica gulosa...\")\n",
    "        df_ranked = self.topological_greedy_sort(df, G)\n",
    "        \n",
    "        print(\"6. Calculando métricas finais...\")\n",
    "        final_metrics = self.calculate_metrics(df_ranked, G)\n",
    "        \n",
    "        # Compilar métricas finais\n",
    "        all_metrics = {\n",
    "            'ml_mae_cv': ml_metrics['mae_cv'],\n",
    "            'n_estimators': ml_metrics['n_estimators'],\n",
    "            'features_used': ml_metrics['features'],\n",
    "            'feature_importance': self.feature_importance,\n",
    "            'inversions_top_20': final_metrics['inversions_top_20'],\n",
    "            'pct_top10_unblocked': final_metrics['pct_top10_unblocked']\n",
    "        }\n",
    "        \n",
    "        return df_ranked, all_metrics\n",
    "    \n",
    "    def save_results(self, df: pd.DataFrame, output_path: str) -> None:\n",
    "        \"\"\"Salva resultados em CSV\"\"\"\n",
    "        # Selecionar e ordenar colunas de saída\n",
    "        output_cols = [\n",
    "            'rank', 'id', 'roi_ajustado', 'dep_impact', 'score_ml',\n",
    "            'deps_ok', 'deps', 'in_degree', 'out_degree',\n",
    "            'degree_centrality', 'story_points'\n",
    "        ]\n",
    "        \n",
    "        # Adicionar colunas extras se existirem\n",
    "        extra_cols = [col for col in df.columns if col not in output_cols and not col.endswith('_encoded')]\n",
    "        output_cols.extend(extra_cols)\n",
    "        \n",
    "        # Filtrar apenas colunas existentes\n",
    "        output_cols = [col for col in output_cols if col in df.columns]\n",
    "        \n",
    "        df[output_cols].to_csv(output_path, index=False)\n",
    "        print(f\"Resultados salvos em: {output_path}\")\n",
    "\n",
    "# Exemplo de uso\n",
    "def main():\n",
    "    \"\"\"Exemplo de execução do algoritmo\"\"\"\n",
    "    import os\n",
    "    \n",
    "\n",
    "    \n",
    "    print(\"=== EXEMPLO DE PRIORIZAÇÃO DE BACKLOG ===\\n\")\n",
    "    \n",
    "    # Inicializar priorizador\n",
    "    prioritizer = BacklogPrioritizer(\n",
    "        decay_factor=0.6,\n",
    "        cv_folds=3,  # Reduzido para dataset pequeno\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Executar priorização\n",
    "        df_result, metrics = prioritizer.prioritize_backlog('/workspaces/codespaces-jupyter/data/trilha_a.csv')\n",
    "        \n",
    "        # Salvar resultados\n",
    "        prioritizer.save_results(df_result, '/workspaces/codespaces-jupyter/data/backlog_priorizado.csv')\n",
    "        \n",
    "        # Exibir resultados\n",
    "        print(\"\\n=== TOP 5 ITENS PRIORIZADOS ===\")\n",
    "        print(df_result[['rank', 'id', 'roi_ajustado', 'score_ml', 'deps_ok', 'story_points']].head())\n",
    "        \n",
    "        print(\"\\n=== MÉTRICAS FINAIS ===\")\n",
    "        print(f\"MAE Cross-Validation: {metrics['ml_mae_cv']:.4f}\")\n",
    "        print(f\"Inversões Top-20: {metrics['inversions_top_20']}\")\n",
    "        print(f\"% Top-10 sem bloqueio: {metrics['pct_top10_unblocked']:.1f}%\")\n",
    "        print(f\"N° de estimadores usados: {metrics['n_estimators']}\")\n",
    "        \n",
    "        print(\"\\n=== IMPORTÂNCIA DAS FEATURES ===\")\n",
    "        for feature, importance in sorted(metrics['feature_importance'].items(), \n",
    "                                        key=lambda x: x[1], reverse=True):\n",
    "            print(f\"{feature}: {importance:.4f}\")\n",
    "        \n",
    "        # Limpar arquivos de exemplo\n",
    "        if os.path.exists('exemplo_backlog.csv'):\n",
    "            os.remove('exemplo_backlog.csv')\n",
    "            \n",
    "        print(f\"\\n✅ Execução completa! Resultados salvos em 'exemplo_backlog_priorizado.csv'\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erro durante execução: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
