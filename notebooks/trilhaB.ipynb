{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5146162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Iniciando planejamento de sprint...\n",
      "1. Carregando arquivos...\n",
      "2. Preparando dados...\n",
      "   Backlog preparado: 28 itens\n",
      "   Ciclo quebrado: removida aresta ('22', '21')\n",
      "   Grafo: 28 n√≥s, 31 arestas\n",
      "3. Treinando modelos preditivos...\n",
      "   Capacidade: Œº=9.2, œÉ=1.9\n",
      "   Modelo overrun treinado (MAE: 0.253)\n",
      "   Spillover rates: {'S': 0.5, 'M': 0.5, 'L': 0.5, 'XL': 0.5}\n",
      "‚úÖ Dados carregados e modelos treinados!\n",
      "4. Executando 1000 simula√ß√µes Monte Carlo...\n",
      "5. Otimizando sele√ß√£o de itens...\n",
      "\n",
      "============================================================\n",
      "üìä PLANEJAMENTO DE SPRINT - TEAM 1\n",
      "============================================================\n",
      "\n",
      "üéØ M√âTRICAS PRINCIPAIS:\n",
      "   Probabilidade de sucesso: 97.1% (meta: 80.0%)\n",
      "   Valor total do sprint: 9.00\n",
      "   Story Points estimados: 6\n",
      "   Story Points efetivos (esperado): 5.5\n",
      "   Capacidade m√©dia do time: 9.2 SP\n",
      "   N√∫mero de itens: 4\n",
      "   Utiliza√ß√£o da capacidade: 59.5% - üü¢ BAIXO\n",
      "\n",
      "üìã ITENS SELECIONADOS (Ordem de Execu√ß√£o):\n",
      "    1. ID 13 | Valor: 2.25 | SP: 3\n",
      "    2. ID 1 | Valor: 2.25 | SP: 1\n",
      "    3. ID 2 | Valor: 2.25 | SP: 1 (deps: ['1'])\n",
      "    4. ID 3 | Valor: 2.25 | SP: 1 (deps: ['1'])\n",
      "\n",
      "üí° RECOMENDA√á√ïES:\n",
      "   ‚úÖ Sprint atende √† meta de confiabilidade\n",
      "\n",
      "üîÑ ITENS DE FRONTEIRA (pr√≥ximos da sele√ß√£o):\n",
      "   ‚Ä¢ ID 4 | Valor: 2.25 | SP: 8\n",
      "   ‚Ä¢ ID 5 | Valor: 2.25 | SP: 8\n",
      "   ‚Ä¢ ID 7 | Valor: 2.25 | SP: 2\n",
      "‚úÖ Resultados salvos em: /workspaces/codespaces-jupyter/data/sprint_planejado.csv\n",
      "‚úÖ M√©tricas salvas em: /workspaces/codespaces-jupyter/data/sprint_planejado_metrics.txt\n",
      "\n",
      "‚úÖ Planejamento conclu√≠do com sucesso!\n",
      "\n",
      "üîç AN√ÅLISE ADICIONAL:\n",
      "   üìå 26 itens com depend√™ncias no backlog\n",
      "   üìä Distribui√ß√£o de tamanhos: {1: np.int64(5), 2: np.int64(3), 3: np.int64(7), 5: np.int64(7), 8: np.int64(6)}\n",
      "   ‚è±Ô∏è  Sprints estimados para todo o backlog: 12.5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Tuple, Set, Optional, Any\n",
    "import networkx as nx\n",
    "from collections import defaultdict, deque\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_absolute_error, brier_score_loss\n",
    "from scipy import stats\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class SprintPlanner:\n",
    "    def __init__(self, \n",
    "                 team_id: int = 1,\n",
    "                 theta: float = 0.8,  # Meta de confiabilidade\n",
    "                 n_simulations: int = 5000,\n",
    "                 lambda_spillover: float = 0.1,  # Penaliza√ß√£o por spillover\n",
    "                 random_state: int = 42):\n",
    "        \"\"\"\n",
    "        Sistema de planejamento de sprint com otimiza√ß√£o probabil√≠stica\n",
    "        \n",
    "        Args:\n",
    "            team_id: ID do time para filtrar dados hist√≥ricos\n",
    "            theta: Meta de confiabilidade (P(on-time) >= theta)\n",
    "            n_simulations: N√∫mero de simula√ß√µes Monte Carlo\n",
    "            lambda_spillover: Fator de penaliza√ß√£o por spillover\n",
    "            random_state: Seed para reprodutibilidade\n",
    "        \"\"\"\n",
    "        self.team_id = team_id\n",
    "        self.theta = theta\n",
    "        self.n_simulations = n_simulations\n",
    "        self.lambda_spillover = lambda_spillover\n",
    "        self.random_state = random_state\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "        # Modelos preditivos\n",
    "        self.capacity_model = None\n",
    "        self.overrun_model = None\n",
    "        self.spillover_model = None\n",
    "        \n",
    "        # Dados processados\n",
    "        self.backlog_df = None\n",
    "        self.dependency_graph = None\n",
    "        self.capacity_distribution = None\n",
    "        self.overrun_distributions = {}\n",
    "        self.spillover_probabilities = {}\n",
    "        \n",
    "    def load_and_process_data(self, \n",
    "                             trilha_b1_path: str,\n",
    "                             trilha_b2_path: str, \n",
    "                             trilha_b3_path: str,\n",
    "                             trilha_b4_path: str,\n",
    "                             backlog_priorizado_path: str) -> None:\n",
    "        \"\"\"Carrega e processa todos os arquivos de entrada\"\"\"\n",
    "        \n",
    "        print(\"1. Carregando arquivos...\")\n",
    "        \n",
    "        # Carregar dados\n",
    "        self.b1_df = pd.read_csv(trilha_b1_path)  # Capacidade hist√≥rica\n",
    "        self.b2_df = pd.read_csv(trilha_b2_path)  # Hist√≥rico por item\n",
    "        self.b3_df = pd.read_csv(trilha_b3_path)  # Capacidade por dev\n",
    "        self.b4_df = pd.read_csv(trilha_b4_path)  # Backlog atual\n",
    "        self.priorizado_df = pd.read_csv(backlog_priorizado_path)  # Sa√≠da Trilha A\n",
    "        \n",
    "        # Normalizar colunas\n",
    "        self._normalize_columns()\n",
    "        \n",
    "        print(\"2. Preparando dados...\")\n",
    "        \n",
    "        # Preparar backlog principal\n",
    "        self._prepare_backlog()\n",
    "        \n",
    "        # Construir grafo de depend√™ncias\n",
    "        self._build_dependency_graph()\n",
    "        \n",
    "        print(\"3. Treinando modelos preditivos...\")\n",
    "        \n",
    "        # Treinar modelos\n",
    "        self._train_capacity_model()\n",
    "        self._train_overrun_model()\n",
    "        self._train_spillover_model()\n",
    "        \n",
    "        print(\"‚úÖ Dados carregados e modelos treinados!\")\n",
    "    \n",
    "    def _normalize_columns(self) -> None:\n",
    "        \"\"\"Normaliza nomes das colunas\"\"\"\n",
    "        # Corrigir coluna problem√°tica em B3\n",
    "        if 'horas disponiveis ' in self.b3_df.columns:\n",
    "            self.b3_df = self.b3_df.rename(columns={'horas disponiveis ': 'horas_disponiveis'})\n",
    "        \n",
    "        # Padronizar nomes\n",
    "        for df in [self.b1_df, self.b2_df, self.b3_df, self.b4_df, self.priorizado_df]:\n",
    "            df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "    \n",
    "    def _prepare_backlog(self) -> None:\n",
    "        \"\"\"Prepara o backlog principal combinando dados das trilhas\"\"\"\n",
    "        # Come√ßar com B4 (backlog atual)\n",
    "        self.backlog_df = self.b4_df.copy()\n",
    "        \n",
    "        # Garantir que issueid seja string para joins\n",
    "        self.backlog_df['issueid'] = self.backlog_df['issueid'].astype(str)\n",
    "        self.priorizado_df['id'] = self.priorizado_df['id'].astype(str)\n",
    "        \n",
    "        # Juntar com dados de valor (Trilha A)\n",
    "        valor_cols = ['id', 'roi_ajustado', 'score_ml', 'dep_impact']\n",
    "        if all(col in self.priorizado_df.columns for col in valor_cols):\n",
    "            valor_df = self.priorizado_df[valor_cols].copy()\n",
    "            self.backlog_df = self.backlog_df.merge(\n",
    "                valor_df, \n",
    "                left_on='issueid', \n",
    "                right_on='id', \n",
    "                how='left'\n",
    "            )\n",
    "            # Usar score_ml calibrado como valor principal\n",
    "            self.backlog_df['valor_trilha_a'] = self.backlog_df['score_ml'].fillna(\n",
    "                self.backlog_df['roi_ajustado'].fillna(\n",
    "                    self.backlog_df['prioridade_score']\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            # Fallback: usar prioridade_score\n",
    "            self.backlog_df['valor_trilha_a'] = self.backlog_df['prioridade_score']\n",
    "        \n",
    "        # Parsear depend√™ncias\n",
    "        self.backlog_df['deps_parsed'] = self.backlog_df['dependencias'].apply(self._parse_dependencies)\n",
    "        \n",
    "        print(f\"   Backlog preparado: {len(self.backlog_df)} itens\")\n",
    "    \n",
    "    def _parse_dependencies(self, deps_str: Any) -> List[str]:\n",
    "        \"\"\"Parseia string de depend√™ncias em lista\"\"\"\n",
    "        if pd.isna(deps_str) or deps_str == '':\n",
    "            return []\n",
    "        \n",
    "        deps_str = str(deps_str).strip()\n",
    "        if not deps_str:\n",
    "            return []\n",
    "        \n",
    "        # Tentar parsear como JSON\n",
    "        try:\n",
    "            if deps_str.startswith('[') and deps_str.endswith(']'):\n",
    "                return json.loads(deps_str)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Parsear com separadores\n",
    "        deps = []\n",
    "        for sep in [',', ';']:\n",
    "            if sep in deps_str:\n",
    "                deps = [dep.strip().strip('\"\\'') for dep in deps_str.split(sep)]\n",
    "                break\n",
    "        else:\n",
    "            deps = [deps_str.strip().strip('\"\\'')]\n",
    "        \n",
    "        return [str(dep) for dep in deps if dep]\n",
    "    \n",
    "    def _build_dependency_graph(self) -> None:\n",
    "        \"\"\"Constr√≥i grafo de depend√™ncias\"\"\"\n",
    "        self.dependency_graph = nx.DiGraph()\n",
    "        \n",
    "        # Todos os IDs v√°lidos\n",
    "        valid_ids = set(self.backlog_df['issueid'].astype(str))\n",
    "        self.dependency_graph.add_nodes_from(valid_ids)\n",
    "        \n",
    "        # Adicionar arestas e detectar depend√™ncias externas\n",
    "        external_deps = defaultdict(list)\n",
    "        \n",
    "        for _, row in self.backlog_df.iterrows():\n",
    "            item_id = str(row['issueid'])\n",
    "            \n",
    "            for dep in row['deps_parsed']:\n",
    "                dep = str(dep)\n",
    "                if dep in valid_ids:\n",
    "                    # Aresta: depend√™ncia ‚Üí item\n",
    "                    self.dependency_graph.add_edge(dep, item_id)\n",
    "                else:\n",
    "                    external_deps[item_id].append(dep)\n",
    "        \n",
    "        # Marcar depend√™ncias externas\n",
    "        self.backlog_df['has_external_deps'] = self.backlog_df['issueid'].astype(str).map(\n",
    "            lambda x: len(external_deps[x]) > 0\n",
    "        )\n",
    "        \n",
    "        # Detectar e quebrar ciclos se necess√°rio\n",
    "        if not nx.is_directed_acyclic_graph(self.dependency_graph):\n",
    "            self._break_cycles()\n",
    "        \n",
    "        print(f\"   Grafo: {len(self.dependency_graph.nodes())} n√≥s, {len(self.dependency_graph.edges())} arestas\")\n",
    "    \n",
    "    def _break_cycles(self) -> None:\n",
    "        \"\"\"Quebra ciclos no grafo\"\"\"\n",
    "        valor_dict = dict(zip(\n",
    "            self.backlog_df['issueid'].astype(str), \n",
    "            self.backlog_df['valor_trilha_a']\n",
    "        ))\n",
    "        \n",
    "        while not nx.is_directed_acyclic_graph(self.dependency_graph):\n",
    "            try:\n",
    "                cycle = nx.find_cycle(self.dependency_graph)\n",
    "                min_weight = float('inf')\n",
    "                edge_to_remove = None\n",
    "                \n",
    "                for u, v in cycle:\n",
    "                    weight = valor_dict.get(u, 0) + valor_dict.get(v, 0)\n",
    "                    if weight < min_weight:\n",
    "                        min_weight = weight\n",
    "                        edge_to_remove = (u, v)\n",
    "                \n",
    "                if edge_to_remove:\n",
    "                    self.dependency_graph.remove_edge(*edge_to_remove)\n",
    "                    print(f\"   Ciclo quebrado: removida aresta {edge_to_remove}\")\n",
    "                else:\n",
    "                    break\n",
    "            except nx.NetworkXNoCycle:\n",
    "                break\n",
    "    \n",
    "    def _train_capacity_model(self) -> None:\n",
    "        \"\"\"Treina modelo de previs√£o de capacidade\"\"\"\n",
    "        # Filtrar por team\n",
    "        team_data = self.b1_df[self.b1_df['teamid'] == self.team_id].copy()\n",
    "        \n",
    "        if len(team_data) < 3:\n",
    "            print(f\"   Poucos dados para team {self.team_id}, usando m√©dia hist√≥rica\")\n",
    "            mean_capacity = team_data['sp_entregues'].mean() if len(team_data) > 0 else 10\n",
    "            std_capacity = team_data['sp_entregues'].std() if len(team_data) > 1 else 2\n",
    "            self.capacity_distribution = {\n",
    "                'mean': mean_capacity,\n",
    "                'std': max(std_capacity, 1.0),\n",
    "                'type': 'normal'\n",
    "            }\n",
    "            return\n",
    "        \n",
    "        # Ordenar por sprint\n",
    "        team_data = team_data.sort_values('sprintid')\n",
    "        ts = team_data['sp_entregues'].values\n",
    "        \n",
    "        try:\n",
    "            # Tentar ETS\n",
    "            model = ExponentialSmoothing(ts, trend='add', seasonal=None)\n",
    "            fitted_model = model.fit()\n",
    "            \n",
    "            # Prever pr√≥ximo valor\n",
    "            forecast = fitted_model.forecast(steps=1)\n",
    "            residuals = fitted_model.resid\n",
    "            \n",
    "            self.capacity_distribution = {\n",
    "                'mean': float(forecast[0]),\n",
    "                'std': float(np.std(residuals)) if len(residuals) > 1 else 2.0,\n",
    "                'type': 'normal'\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Fallback: usar m√©dia e desvio hist√≥ricos\n",
    "            self.capacity_distribution = {\n",
    "                'mean': float(np.mean(ts)),\n",
    "                'std': float(np.std(ts)) if len(ts) > 1 else 2.0,\n",
    "                'type': 'normal'\n",
    "            }\n",
    "        \n",
    "        print(f\"   Capacidade: Œº={self.capacity_distribution['mean']:.1f}, œÉ={self.capacity_distribution['std']:.1f}\")\n",
    "    \n",
    "    def _train_overrun_model(self) -> None:\n",
    "        \"\"\"Treina modelo de overrun (raz√£o realizado/estimado)\"\"\"\n",
    "        # Preparar dados de overrun\n",
    "        self.b2_df['overrun_ratio'] = self.b2_df['storypoints_realizados'] / self.b2_df['storypoints_estimados']\n",
    "        self.b2_df['log_overrun'] = np.log(self.b2_df['overrun_ratio'])\n",
    "        \n",
    "        # Features dispon√≠veis\n",
    "        features = []\n",
    "        X_data = pd.DataFrame()\n",
    "        \n",
    "        # SP estimados (binned)\n",
    "        X_data['sp_estimados'] = self.b2_df['storypoints_estimados']\n",
    "        X_data['sp_bin'] = pd.cut(X_data['sp_estimados'], bins=[0, 2, 5, 8, float('inf')], labels=[0, 1, 2, 3])\n",
    "        features.extend(['sp_estimados', 'sp_bin'])\n",
    "        \n",
    "        # Spillover como feature\n",
    "        if 'spillover_(dep_externa)' in self.b2_df.columns:\n",
    "            X_data['spillover'] = self.b2_df['spillover_(dep_externa)']\n",
    "            features.append('spillover')\n",
    "        \n",
    "        # Alvo\n",
    "        y = self.b2_df['log_overrun'].fillna(0)\n",
    "        \n",
    "        if len(X_data) > 10:\n",
    "            try:\n",
    "                # Treinar LightGBM\n",
    "                self.overrun_model = lgb.LGBMRegressor(\n",
    "                    n_estimators=100,\n",
    "                    random_state=self.random_state,\n",
    "                    verbose=-1\n",
    "                )\n",
    "                self.overrun_model.fit(X_data[features], y)\n",
    "                \n",
    "                # Calcular res√≠duos para incerteza\n",
    "                predictions = self.overrun_model.predict(X_data[features])\n",
    "                residuals = y - predictions\n",
    "                residual_std = np.std(residuals)\n",
    "                \n",
    "                print(f\"   Modelo overrun treinado (MAE: {mean_absolute_error(y, predictions):.3f})\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   Erro no modelo overrun, usando m√©dias: {e}\")\n",
    "                self.overrun_model = None\n",
    "                residual_std = np.std(y)\n",
    "        else:\n",
    "            self.overrun_model = None\n",
    "            residual_std = np.std(y) if len(y) > 1 else 0.2\n",
    "        \n",
    "        # Distribui√ß√µes por faixas de SP\n",
    "        for sp_range in [(0, 2), (2, 5), (5, 8), (8, float('inf'))]:\n",
    "            mask = (self.b2_df['storypoints_estimados'] >= sp_range[0]) & \\\n",
    "                   (self.b2_df['storypoints_estimados'] < sp_range[1])\n",
    "            \n",
    "            if mask.sum() > 0:\n",
    "                range_data = self.b2_df[mask]['log_overrun']\n",
    "                self.overrun_distributions[sp_range] = {\n",
    "                    'mean': float(range_data.mean()),\n",
    "                    'std': max(float(range_data.std()), residual_std, 0.1)\n",
    "                }\n",
    "            else:\n",
    "                self.overrun_distributions[sp_range] = {\n",
    "                    'mean': 0.0,\n",
    "                    'std': residual_std\n",
    "                }\n",
    "    \n",
    "    def _train_spillover_model(self) -> None:\n",
    "        \"\"\"Treina modelo de probabilidade de spillover\"\"\"\n",
    "        if 'spillover_(dep_externa)' not in self.b2_df.columns:\n",
    "            # Criar probabilidades base por faixa de SP\n",
    "            for sp_range in [(0, 2), (2, 5), (5, 8), (8, float('inf'))]:\n",
    "                self.spillover_probabilities[sp_range] = 0.1  # 10% base\n",
    "            return\n",
    "        \n",
    "        # Agrupar por faixas de SP e presen√ßa de dep externa\n",
    "        self.b2_df['sp_bin'] = pd.cut(\n",
    "            self.b2_df['storypoints_estimados'], \n",
    "            bins=[0, 2, 5, 8, float('inf')],\n",
    "            labels=['small', 'medium', 'large', 'xlarge']\n",
    "        )\n",
    "        \n",
    "        # Calcular probabilidades por faixa\n",
    "        spillover_rates = self.b2_df.groupby('sp_bin')['spillover_(dep_externa)'].mean()\n",
    "        \n",
    "        for i, sp_range in enumerate([(0, 2), (2, 5), (5, 8), (8, float('inf'))]):\n",
    "            bin_label = ['small', 'medium', 'large', 'xlarge'][i]\n",
    "            if bin_label in spillover_rates.index:\n",
    "                self.spillover_probabilities[sp_range] = max(0.05, min(0.5, spillover_rates[bin_label]))\n",
    "            else:\n",
    "                self.spillover_probabilities[sp_range] = 0.1\n",
    "        \n",
    "        print(f\"   Spillover rates: {dict(zip(['S', 'M', 'L', 'XL'], self.spillover_probabilities.values()))}\")\n",
    "    \n",
    "    def _get_sp_range(self, sp: float) -> Tuple[float, float]:\n",
    "        \"\"\"Retorna a faixa de SP para um valor\"\"\"\n",
    "        for sp_range in [(0, 2), (2, 5), (5, 8), (8, float('inf'))]:\n",
    "            if sp >= sp_range[0] and sp < sp_range[1]:\n",
    "                return sp_range\n",
    "        return (8, float('inf'))\n",
    "    \n",
    "    def monte_carlo_simulation(self) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Executa simula√ß√£o Monte Carlo\"\"\"\n",
    "        print(f\"4. Executando {self.n_simulations} simula√ß√µes Monte Carlo...\")\n",
    "        \n",
    "        results = {\n",
    "            'capacity_samples': np.zeros(self.n_simulations),\n",
    "            'overrun_samples': {},  # Por item\n",
    "            'sp_efetivo_samples': {}  # Por item\n",
    "        }\n",
    "        \n",
    "        # Amostrar capacidades\n",
    "        results['capacity_samples'] = np.random.normal(\n",
    "            self.capacity_distribution['mean'],\n",
    "            self.capacity_distribution['std'],\n",
    "            self.n_simulations\n",
    "        )\n",
    "        results['capacity_samples'] = np.maximum(results['capacity_samples'], 1)  # M√≠nimo 1 SP\n",
    "        \n",
    "        # Amostrar overrun por item\n",
    "        for _, item in self.backlog_df.iterrows():\n",
    "            item_id = str(item['issueid'])\n",
    "            sp = item['storypoints']\n",
    "            sp_range = self._get_sp_range(sp)\n",
    "            \n",
    "            if self.overrun_model is not None:\n",
    "                # Usar modelo se dispon√≠vel\n",
    "                try:\n",
    "                    item_features = pd.DataFrame({\n",
    "                        'sp_estimados': [sp],\n",
    "                        'sp_bin': [self._get_sp_bin(sp)],\n",
    "                        'spillover': [int(item.get('has_external_deps', False))]\n",
    "                    })\n",
    "                    \n",
    "                    log_overrun_pred = self.overrun_model.predict(item_features)[0]\n",
    "                    log_overrun_std = self.overrun_distributions[sp_range]['std']\n",
    "                    \n",
    "                    log_overrun_samples = np.random.normal(\n",
    "                        log_overrun_pred, \n",
    "                        log_overrun_std, \n",
    "                        self.n_simulations\n",
    "                    )\n",
    "                except:\n",
    "                    # Fallback para distribui√ß√£o por faixa\n",
    "                    dist = self.overrun_distributions[sp_range]\n",
    "                    log_overrun_samples = np.random.normal(\n",
    "                        dist['mean'], \n",
    "                        dist['std'], \n",
    "                        self.n_simulations\n",
    "                    )\n",
    "            else:\n",
    "                # Usar distribui√ß√£o por faixa de SP\n",
    "                dist = self.overrun_distributions[sp_range]\n",
    "                log_overrun_samples = np.random.normal(\n",
    "                    dist['mean'], \n",
    "                    dist['std'], \n",
    "                    self.n_simulations\n",
    "                )\n",
    "            \n",
    "            # Converter para overrun ratio\n",
    "            overrun_samples = np.exp(log_overrun_samples)\n",
    "            overrun_samples = np.maximum(overrun_samples, 0.5)  # M√≠nimo 50% do estimado\n",
    "            overrun_samples = np.minimum(overrun_samples, 3.0)  # M√°ximo 300% do estimado\n",
    "            \n",
    "            results['overrun_samples'][item_id] = overrun_samples\n",
    "            results['sp_efetivo_samples'][item_id] = sp * overrun_samples\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _get_sp_bin(self, sp: float) -> int:\n",
    "        \"\"\"Converte SP em bin categ√≥rico\"\"\"\n",
    "        if sp <= 2:\n",
    "            return 0\n",
    "        elif sp <= 5:\n",
    "            return 1\n",
    "        elif sp <= 8:\n",
    "            return 2\n",
    "        else:\n",
    "            return 3\n",
    "    \n",
    "    def optimize_sprint(self, simulation_results: Dict) -> Tuple[List[str], Dict]:\n",
    "        \"\"\"Otimiza sele√ß√£o de itens para o sprint\"\"\"\n",
    "        print(\"5. Otimizando sele√ß√£o de itens...\")\n",
    "        \n",
    "        # Algoritmo guloso com chance constraints\n",
    "        best_sprint = []\n",
    "        best_value = 0\n",
    "        best_prob_on_time = 0\n",
    "        \n",
    "        # Criar lista de candidatos ordenados por ganho marginal esperado\n",
    "        candidates = []\n",
    "        \n",
    "        for _, item in self.backlog_df.iterrows():\n",
    "            item_id = str(item['issueid'])\n",
    "            sp_range = self._get_sp_range(item['storypoints'])\n",
    "            spillover_prob = self.spillover_probabilities.get(sp_range, 0.1)\n",
    "            \n",
    "            # Penaliza√ß√£o por spillover\n",
    "            spillover_penalty = 1.0 - self.lambda_spillover * spillover_prob\n",
    "            if item.get('has_external_deps', False):\n",
    "                spillover_penalty *= 0.8  # Penaliza√ß√£o adicional por dep externa\n",
    "            \n",
    "            # Ganho marginal esperado\n",
    "            expected_sp_efetivo = np.mean(simulation_results['sp_efetivo_samples'][item_id])\n",
    "            marginal_gain = (item['valor_trilha_a'] * spillover_penalty) / expected_sp_efetivo\n",
    "            \n",
    "            candidates.append({\n",
    "                'id': item_id,\n",
    "                'value': item['valor_trilha_a'],\n",
    "                'marginal_gain': marginal_gain,\n",
    "                'sp_estimado': item['storypoints'],\n",
    "                'deps': item['deps_parsed']\n",
    "            })\n",
    "        \n",
    "        # Ordenar por ganho marginal\n",
    "        candidates.sort(key=lambda x: x['marginal_gain'], reverse=True)\n",
    "        \n",
    "        # Sele√ß√£o gulosa respeitando depend√™ncias\n",
    "        current_sprint = self._greedy_selection_with_dependencies(\n",
    "            candidates, simulation_results\n",
    "        )\n",
    "        \n",
    "        # Calcular probabilidade de sucesso\n",
    "        prob_on_time = self._calculate_success_probability(current_sprint, simulation_results)\n",
    "        \n",
    "        # Ajustar para atingir meta theta\n",
    "        if prob_on_time < self.theta:\n",
    "            current_sprint = self._adjust_for_reliability(\n",
    "                current_sprint, simulation_results\n",
    "            )\n",
    "            prob_on_time = self._calculate_success_probability(current_sprint, simulation_results)\n",
    "        \n",
    "        # M√©tricas finais\n",
    "        total_value = sum(self.backlog_df[self.backlog_df['issueid'].astype(str).isin(current_sprint)]['valor_trilha_a'])\n",
    "        total_sp_estimado = sum(self.backlog_df[self.backlog_df['issueid'].astype(str).isin(current_sprint)]['storypoints'])\n",
    "        \n",
    "        expected_sp_efetivo = sum(\n",
    "            np.mean(simulation_results['sp_efetivo_samples'][item_id])\n",
    "            for item_id in current_sprint\n",
    "        )\n",
    "        \n",
    "        metrics = {\n",
    "            'prob_on_time': prob_on_time,\n",
    "            'total_value': total_value,\n",
    "            'total_sp_estimado': total_sp_estimado,\n",
    "            'expected_sp_efetivo': expected_sp_efetivo,\n",
    "            'n_items': len(current_sprint),\n",
    "            'capacity_mean': self.capacity_distribution['mean']\n",
    "        }\n",
    "        \n",
    "        return current_sprint, metrics\n",
    "    \n",
    "    def _greedy_selection_with_dependencies(self, candidates: List[Dict], \n",
    "                                           simulation_results: Dict) -> List[str]:\n",
    "        \"\"\"Sele√ß√£o gulosa respeitando depend√™ncias\"\"\"\n",
    "        selected = []\n",
    "        G = self.dependency_graph.copy()\n",
    "        \n",
    "        while candidates:\n",
    "            # Encontrar candidatos sem depend√™ncias n√£o satisfeitas\n",
    "            available = []\n",
    "            for candidate in candidates:\n",
    "                item_id = candidate['id']\n",
    "                # Verificar se todas as depend√™ncias j√° est√£o selecionadas\n",
    "                deps_satisfied = all(\n",
    "                    dep in selected or dep not in G.nodes()\n",
    "                    for dep in candidate['deps']\n",
    "                )\n",
    "                if deps_satisfied:\n",
    "                    available.append(candidate)\n",
    "            \n",
    "            if not available:\n",
    "                break\n",
    "            \n",
    "            # Escolher melhor dispon√≠vel\n",
    "            best_candidate = max(available, key=lambda x: x['marginal_gain'])\n",
    "            item_id = best_candidate['id']\n",
    "            \n",
    "            # Verificar se cabe no sprint (usando percentil 80 da capacidade)\n",
    "            capacity_p80 = np.percentile(simulation_results['capacity_samples'], 80)\n",
    "            current_sp_efetivo = sum(\n",
    "                np.percentile(simulation_results['sp_efetivo_samples'][sel_id], 80)\n",
    "                for sel_id in selected\n",
    "            )\n",
    "            item_sp_efetivo = np.percentile(simulation_results['sp_efetivo_samples'][item_id], 80)\n",
    "            \n",
    "            if current_sp_efetivo + item_sp_efetivo <= capacity_p80:\n",
    "                selected.append(item_id)\n",
    "            \n",
    "            # Remover candidato da lista\n",
    "            candidates.remove(best_candidate)\n",
    "        \n",
    "        return selected\n",
    "    \n",
    "    def _calculate_success_probability(self, sprint_items: List[str], \n",
    "                                     simulation_results: Dict) -> float:\n",
    "        \"\"\"Calcula P(on-time) para um sprint\"\"\"\n",
    "        if not sprint_items:\n",
    "            return 1.0\n",
    "        \n",
    "        successes = 0\n",
    "        for i in range(self.n_simulations):\n",
    "            capacity = simulation_results['capacity_samples'][i]\n",
    "            total_sp_efetivo = sum(\n",
    "                simulation_results['sp_efetivo_samples'][item_id][i]\n",
    "                for item_id in sprint_items\n",
    "            )\n",
    "            \n",
    "            if total_sp_efetivo <= capacity:\n",
    "                successes += 1\n",
    "        \n",
    "        return successes / self.n_simulations\n",
    "    \n",
    "    def _adjust_for_reliability(self, initial_sprint: List[str], \n",
    "                               simulation_results: Dict) -> List[str]:\n",
    "        \"\"\"Ajusta sprint para atingir meta de confiabilidade\"\"\"\n",
    "        current_sprint = initial_sprint.copy()\n",
    "        \n",
    "        while (self._calculate_success_probability(current_sprint, simulation_results) < self.theta \n",
    "               and len(current_sprint) > 0):\n",
    "            \n",
    "            # Remover item com menor ganho marginal\n",
    "            worst_item = None\n",
    "            worst_gain = float('inf')\n",
    "            \n",
    "            for item_id in current_sprint:\n",
    "                item_data = self.backlog_df[self.backlog_df['issueid'].astype(str) == item_id].iloc[0]\n",
    "                expected_sp = np.mean(simulation_results['sp_efetivo_samples'][item_id])\n",
    "                gain = item_data['valor_trilha_a'] / expected_sp\n",
    "                \n",
    "                if gain < worst_gain:\n",
    "                    worst_gain = gain\n",
    "                    worst_item = item_id\n",
    "            \n",
    "            if worst_item:\n",
    "                current_sprint.remove(worst_item)\n",
    "        \n",
    "        return current_sprint\n",
    "    \n",
    "    def generate_execution_order(self, sprint_items: List[str]) -> List[str]:\n",
    "        \"\"\"Gera ordem de execu√ß√£o respeitando depend√™ncias\"\"\"\n",
    "        if not sprint_items:\n",
    "            return []\n",
    "        \n",
    "        # Subgrafo apenas com itens do sprint\n",
    "        subgraph = self.dependency_graph.subgraph(sprint_items).copy()\n",
    "        \n",
    "        # Ordena√ß√£o topol√≥gica com prioriza√ß√£o por valor\n",
    "        ordered = []\n",
    "        while subgraph.nodes():\n",
    "            # Encontrar n√≥s sem depend√™ncias\n",
    "            available = [node for node in subgraph.nodes() if subgraph.in_degree(node) == 0]\n",
    "            \n",
    "            if not available:\n",
    "                # Se h√° ciclos, pegar qualquer n√≥\n",
    "                available = list(subgraph.nodes())\n",
    "            \n",
    "            # Escolher por maior valor\n",
    "            best_item = max(available, key=lambda x: \n",
    "                self.backlog_df[self.backlog_df['issueid'].astype(str) == x]['valor_trilha_a'].iloc[0]\n",
    "            )\n",
    "            \n",
    "            ordered.append(best_item)\n",
    "            subgraph.remove_node(best_item)\n",
    "        \n",
    "        return ordered\n",
    "    \n",
    "    def plan_sprint(self, \n",
    "                   trilha_b1_path: str = \"/workspaces/codespaces-jupyter/data/Backlog - trilhaB1.csv\",\n",
    "                   trilha_b2_path: str = \"/workspaces/codespaces-jupyter/data/Backlog - trilhaB2.csv\",\n",
    "                   trilha_b3_path: str = \"/workspaces/codespaces-jupyter/data/Backlog - trilhaB3.csv\", \n",
    "                   trilha_b4_path: str = \"/workspaces/codespaces-jupyter/data/Backlog - trilhaB4.csv\",\n",
    "                   backlog_priorizado_path: str = \"/workspaces/codespaces-jupyter/data/backlog_priorizado.csv\") -> Dict:\n",
    "        \"\"\"Executa todo o pipeline de planejamento\"\"\"\n",
    "        \n",
    "        # Carregar dados\n",
    "        self.load_and_process_data(\n",
    "            trilha_b1_path, trilha_b2_path, trilha_b3_path,\n",
    "            trilha_b4_path, backlog_priorizado_path\n",
    "        )\n",
    "        \n",
    "        # Simula√ß√£o Monte Carlo\n",
    "        simulation_results = self.monte_carlo_simulation()\n",
    "        \n",
    "        # Otimiza√ß√£o\n",
    "        sprint_items, metrics = self.optimize_sprint(simulation_results)\n",
    "        \n",
    "        # Ordem de execu√ß√£o\n",
    "        execution_order = self.generate_execution_order(sprint_items)\n",
    "        \n",
    "        # Compilar resultado final\n",
    "        result = {\n",
    "            'sprint_items': sprint_items,\n",
    "            'execution_order': execution_order,\n",
    "            'metrics': metrics,\n",
    "            'items_details': self.backlog_df[\n",
    "                self.backlog_df['issueid'].astype(str).isin(sprint_items)\n",
    "            ][['issueid', 'valor_trilha_a', 'storypoints', 'deps_parsed']].to_dict('records')\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def print_results(self, result: Dict) -> None:\n",
    "        \"\"\"Imprime resultados de forma organizada\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"üìä PLANEJAMENTO DE SPRINT - TEAM {self.team_id}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        metrics = result['metrics']\n",
    "        \n",
    "        # M√©tricas principais\n",
    "        print(f\"\\nüéØ M√âTRICAS PRINCIPAIS:\")\n",
    "        print(f\"   Probabilidade de sucesso: {metrics['prob_on_time']:.1%} (meta: {self.theta:.1%})\")\n",
    "        print(f\"   Valor total do sprint: {metrics['total_value']:.2f}\")\n",
    "        print(f\"   Story Points estimados: {metrics['total_sp_estimado']}\")\n",
    "        print(f\"   Story Points efetivos (esperado): {metrics['expected_sp_efetivo']:.1f}\")\n",
    "        print(f\"   Capacidade m√©dia do time: {metrics['capacity_mean']:.1f} SP\")\n",
    "        print(f\"   N√∫mero de itens: {metrics['n_items']}\")\n",
    "        \n",
    "        # Indicadores de risco\n",
    "        utilization = metrics['expected_sp_efetivo'] / metrics['capacity_mean']\n",
    "        risk_level = \"üü¢ BAIXO\" if utilization < 0.7 else \"üü° M√âDIO\" if utilization < 0.9 else \"üî¥ ALTO\"\n",
    "        print(f\"   Utiliza√ß√£o da capacidade: {utilization:.1%} - {risk_level}\")\n",
    "        \n",
    "        # Lista de itens selecionados\n",
    "        print(f\"\\nüìã ITENS SELECIONADOS (Ordem de Execu√ß√£o):\")\n",
    "        for i, item_id in enumerate(result['execution_order'], 1):\n",
    "            item_detail = next(\n",
    "                (item for item in result['items_details'] if str(item['issueid']) == item_id), \n",
    "                None\n",
    "            )\n",
    "            if item_detail:\n",
    "                deps_str = f\" (deps: {item_detail['deps_parsed']})\" if item_detail['deps_parsed'] else \"\"\n",
    "                print(f\"   {i:2d}. ID {item_id} | Valor: {item_detail['valor_trilha_a']:.2f} | SP: {item_detail['storypoints']}{deps_str}\")\n",
    "        \n",
    "        # Recomenda√ß√µes\n",
    "        print(f\"\\nüí° RECOMENDA√á√ïES:\")\n",
    "        if metrics['prob_on_time'] < self.theta:\n",
    "            print(f\"   ‚ö†Ô∏è  Probabilidade de sucesso abaixo da meta ({self.theta:.1%})\")\n",
    "            print(f\"   üìâ Considere remover itens ou revisar estimativas\")\n",
    "        else:\n",
    "            print(f\"   ‚úÖ Sprint atende √† meta de confiabilidade\")\n",
    "        \n",
    "        if utilization > 0.85:\n",
    "            print(f\"   ‚ö†Ô∏è  Alta utiliza√ß√£o da capacidade - risco de spillover\")\n",
    "        \n",
    "        # Itens de fronteira (n√£o selecionados mas com alta prioridade)\n",
    "        print(f\"\\nüîÑ ITENS DE FRONTEIRA (pr√≥ximos da sele√ß√£o):\")\n",
    "        selected_ids = set(result['sprint_items'])\n",
    "        frontier_items = self.backlog_df[\n",
    "            ~self.backlog_df['issueid'].astype(str).isin(selected_ids)\n",
    "        ].nlargest(3, 'valor_trilha_a')\n",
    "        \n",
    "        for _, item in frontier_items.iterrows():\n",
    "            print(f\"   ‚Ä¢ ID {item['issueid']} | Valor: {item['valor_trilha_a']:.2f} | SP: {item['storypoints']}\")\n",
    "\n",
    "    def save_results(self, result: Dict, output_path: str) -> None:\n",
    "        \"\"\"Salva resultados em CSV\"\"\"\n",
    "        # Preparar dados para salvar\n",
    "        output_data = []\n",
    "        \n",
    "        for i, item_id in enumerate(result['execution_order'], 1):\n",
    "            item_detail = next(\n",
    "                (item for item in result['items_details'] if str(item['issueid']) == item_id),\n",
    "                None\n",
    "            )\n",
    "            \n",
    "            if item_detail:\n",
    "                output_data.append({\n",
    "                    'execution_order': i,\n",
    "                    'item_id': item_id,\n",
    "                    'valor_trilha_a': item_detail['valor_trilha_a'],\n",
    "                    'story_points': item_detail['storypoints'],\n",
    "                    'dependencies': str(item_detail['deps_parsed']),\n",
    "                    'has_dependencies': len(item_detail['deps_parsed']) > 0\n",
    "                })\n",
    "        \n",
    "        # Adicionar m√©tricas como linhas extras\n",
    "        metrics = result['metrics']\n",
    "        \n",
    "        # Salvar\n",
    "        df_output = pd.DataFrame(output_data)\n",
    "        df_output.to_csv(output_path, index=False)\n",
    "        \n",
    "        # Salvar m√©tricas em arquivo separado\n",
    "        metrics_path = output_path.replace('.csv', '_metrics.txt')\n",
    "        with open(metrics_path, 'w') as f:\n",
    "            f.write(f\"PLANEJAMENTO DE SPRINT - TEAM {self.team_id}\\n\")\n",
    "            f.write(\"=\"*50 + \"\\n\\n\")\n",
    "            f.write(f\"Probabilidade de sucesso: {metrics['prob_on_time']:.1%}\\n\")\n",
    "            f.write(f\"Valor total: {metrics['total_value']:.2f}\\n\")\n",
    "            f.write(f\"SP estimados: {metrics['total_sp_estimado']}\\n\")\n",
    "            f.write(f\"SP efetivos esperados: {metrics['expected_sp_efetivo']:.1f}\\n\")\n",
    "            f.write(f\"Capacidade m√©dia: {metrics['capacity_mean']:.1f}\\n\")\n",
    "            f.write(f\"N√∫mero de itens: {metrics['n_items']}\\n\")\n",
    "            f.write(f\"Utiliza√ß√£o: {metrics['expected_sp_efetivo']/metrics['capacity_mean']:.1%}\\n\")\n",
    "        \n",
    "        print(f\"‚úÖ Resultados salvos em: {output_path}\")\n",
    "        print(f\"‚úÖ M√©tricas salvas em: {metrics_path}\")\n",
    "\n",
    "# Fun√ß√£o principal para execu√ß√£o\n",
    "def main():\n",
    "    \"\"\"Exemplo de execu√ß√£o do planejamento de sprint\"\"\"\n",
    "    \n",
    "    # Par√¢metros do planejador\n",
    "    planner = SprintPlanner(\n",
    "        team_id=1,\n",
    "        theta=0.8,  # 80% de confiabilidade\n",
    "        n_simulations=1000,  # Reduzido para exemplo r√°pido\n",
    "        lambda_spillover=0.1,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        print(\"üöÄ Iniciando planejamento de sprint...\")\n",
    "        \n",
    "        # Executar planejamento completo\n",
    "        result = planner.plan_sprint()\n",
    "        \n",
    "        # Mostrar resultados\n",
    "        planner.print_results(result)\n",
    "        \n",
    "        # Salvar resultados\n",
    "        planner.save_results(result, '/workspaces/codespaces-jupyter/data/sprint_planejado.csv')\n",
    "        \n",
    "        print(\"\\n‚úÖ Planejamento conclu√≠do com sucesso!\")\n",
    "        \n",
    "        # An√°lise adicional\n",
    "        print(f\"\\nüîç AN√ÅLISE ADICIONAL:\")\n",
    "        \n",
    "        # Verificar se h√° itens bloqueados\n",
    "        blocked_items = planner.backlog_df[\n",
    "            planner.backlog_df['deps_parsed'].apply(len) > 0\n",
    "        ]\n",
    "        if len(blocked_items) > 0:\n",
    "            print(f\"   üìå {len(blocked_items)} itens com depend√™ncias no backlog\")\n",
    "        \n",
    "        # Distribui√ß√£o de tamanhos\n",
    "        size_dist = planner.backlog_df['storypoints'].value_counts().sort_index()\n",
    "        print(f\"   üìä Distribui√ß√£o de tamanhos: {dict(size_dist)}\")\n",
    "        \n",
    "        # Capacidade vs demanda\n",
    "        total_backlog_sp = planner.backlog_df['storypoints'].sum()\n",
    "        sprints_needed = total_backlog_sp / planner.capacity_distribution['mean']\n",
    "        print(f\"   ‚è±Ô∏è  Sprints estimados para todo o backlog: {sprints_needed:.1f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro durante planejamento: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Classe utilit√°ria para an√°lise de sensibilidade\n",
    "class SensitivityAnalyzer:\n",
    "    \"\"\"An√°lise de sensibilidade dos par√¢metros do planejamento\"\"\"\n",
    "    \n",
    "    def __init__(self, planner: SprintPlanner):\n",
    "        self.planner = planner\n",
    "    \n",
    "    def analyze_theta_sensitivity(self, theta_range: List[float]) -> pd.DataFrame:\n",
    "        \"\"\"Analisa como diferentes valores de theta afetam o sprint\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        original_theta = self.planner.theta\n",
    "        simulation_results = self.planner.monte_carlo_simulation()\n",
    "        \n",
    "        for theta in theta_range:\n",
    "            self.planner.theta = theta\n",
    "            sprint_items, metrics = self.planner.optimize_sprint(simulation_results)\n",
    "            \n",
    "            results.append({\n",
    "                'theta': theta,\n",
    "                'n_items': metrics['n_items'],\n",
    "                'total_value': metrics['total_value'],\n",
    "                'prob_on_time': metrics['prob_on_time'],\n",
    "                'sp_estimado': metrics['total_sp_estimado'],\n",
    "                'sp_efetivo': metrics['expected_sp_efetivo']\n",
    "            })\n",
    "        \n",
    "        # Restaurar theta original\n",
    "        self.planner.theta = original_theta\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "    \n",
    "    def analyze_capacity_impact(self, capacity_adjustments: List[float]) -> pd.DataFrame:\n",
    "        \"\"\"Analisa impacto de diferentes cen√°rios de capacidade\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        original_capacity = self.planner.capacity_distribution.copy()\n",
    "        \n",
    "        for adjustment in capacity_adjustments:\n",
    "            # Ajustar capacidade\n",
    "            self.planner.capacity_distribution['mean'] = original_capacity['mean'] * adjustment\n",
    "            \n",
    "            # Re-executar simula√ß√£o e otimiza√ß√£o\n",
    "            simulation_results = self.planner.monte_carlo_simulation()\n",
    "            sprint_items, metrics = self.planner.optimize_sprint(simulation_results)\n",
    "            \n",
    "            results.append({\n",
    "                'capacity_factor': adjustment,\n",
    "                'capacity_mean': self.planner.capacity_distribution['mean'],\n",
    "                'n_items': metrics['n_items'],\n",
    "                'total_value': metrics['total_value'],\n",
    "                'prob_on_time': metrics['prob_on_time']\n",
    "            })\n",
    "        \n",
    "        # Restaurar capacidade original\n",
    "        self.planner.capacity_distribution = original_capacity\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
