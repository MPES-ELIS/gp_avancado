{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5146162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Iniciando planejamento de sprint...\n",
      "1. Carregando arquivos...\n",
      "2. Preparando dados...\n",
      "   Backlog preparado: 28 itens\n",
      "   Ciclo quebrado: removida aresta ('22', '21')\n",
      "   Grafo: 28 nós, 31 arestas\n",
      "3. Treinando modelos preditivos...\n",
      "   Capacidade: μ=9.2, σ=1.9\n",
      "   Modelo overrun treinado (MAE: 0.253)\n",
      "   Spillover rates: {'S': 0.5, 'M': 0.5, 'L': 0.5, 'XL': 0.5}\n",
      "✅ Dados carregados e modelos treinados!\n",
      "4. Executando 1000 simulações Monte Carlo...\n",
      "5. Otimizando seleção de itens...\n",
      "\n",
      "============================================================\n",
      "📊 PLANEJAMENTO DE SPRINT - TEAM 1\n",
      "============================================================\n",
      "\n",
      "🎯 MÉTRICAS PRINCIPAIS:\n",
      "   Probabilidade de sucesso: 97.1% (meta: 80.0%)\n",
      "   Valor total do sprint: 9.00\n",
      "   Story Points estimados: 6\n",
      "   Story Points efetivos (esperado): 5.5\n",
      "   Capacidade média do time: 9.2 SP\n",
      "   Número de itens: 4\n",
      "   Utilização da capacidade: 59.5% - 🟢 BAIXO\n",
      "\n",
      "📋 ITENS SELECIONADOS (Ordem de Execução):\n",
      "    1. ID 13 | Valor: 2.25 | SP: 3\n",
      "    2. ID 1 | Valor: 2.25 | SP: 1\n",
      "    3. ID 2 | Valor: 2.25 | SP: 1 (deps: ['1'])\n",
      "    4. ID 3 | Valor: 2.25 | SP: 1 (deps: ['1'])\n",
      "\n",
      "💡 RECOMENDAÇÕES:\n",
      "   ✅ Sprint atende à meta de confiabilidade\n",
      "\n",
      "🔄 ITENS DE FRONTEIRA (próximos da seleção):\n",
      "   • ID 4 | Valor: 2.25 | SP: 8\n",
      "   • ID 5 | Valor: 2.25 | SP: 8\n",
      "   • ID 7 | Valor: 2.25 | SP: 2\n",
      "✅ Resultados salvos em: /workspaces/codespaces-jupyter/data/sprint_planejado.csv\n",
      "✅ Métricas salvas em: /workspaces/codespaces-jupyter/data/sprint_planejado_metrics.txt\n",
      "\n",
      "✅ Planejamento concluído com sucesso!\n",
      "\n",
      "🔍 ANÁLISE ADICIONAL:\n",
      "   📌 26 itens com dependências no backlog\n",
      "   📊 Distribuição de tamanhos: {1: np.int64(5), 2: np.int64(3), 3: np.int64(7), 5: np.int64(7), 8: np.int64(6)}\n",
      "   ⏱️  Sprints estimados para todo o backlog: 12.5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Tuple, Set, Optional, Any\n",
    "import networkx as nx\n",
    "from collections import defaultdict, deque\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_absolute_error, brier_score_loss\n",
    "from scipy import stats\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class SprintPlanner:\n",
    "    def __init__(self, \n",
    "                 team_id: int = 1,\n",
    "                 theta: float = 0.8,  # Meta de confiabilidade\n",
    "                 n_simulations: int = 5000,\n",
    "                 lambda_spillover: float = 0.1,  # Penalização por spillover\n",
    "                 random_state: int = 42):\n",
    "        \"\"\"\n",
    "        Sistema de planejamento de sprint com otimização probabilística\n",
    "        \n",
    "        Args:\n",
    "            team_id: ID do time para filtrar dados históricos\n",
    "            theta: Meta de confiabilidade (P(on-time) >= theta)\n",
    "            n_simulations: Número de simulações Monte Carlo\n",
    "            lambda_spillover: Fator de penalização por spillover\n",
    "            random_state: Seed para reprodutibilidade\n",
    "        \"\"\"\n",
    "        self.team_id = team_id\n",
    "        self.theta = theta\n",
    "        self.n_simulations = n_simulations\n",
    "        self.lambda_spillover = lambda_spillover\n",
    "        self.random_state = random_state\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "        # Modelos preditivos\n",
    "        self.capacity_model = None\n",
    "        self.overrun_model = None\n",
    "        self.spillover_model = None\n",
    "        \n",
    "        # Dados processados\n",
    "        self.backlog_df = None\n",
    "        self.dependency_graph = None\n",
    "        self.capacity_distribution = None\n",
    "        self.overrun_distributions = {}\n",
    "        self.spillover_probabilities = {}\n",
    "        \n",
    "    def load_and_process_data(self, \n",
    "                             trilha_b1_path: str,\n",
    "                             trilha_b2_path: str, \n",
    "                             trilha_b3_path: str,\n",
    "                             trilha_b4_path: str,\n",
    "                             backlog_priorizado_path: str) -> None:\n",
    "        \"\"\"Carrega e processa todos os arquivos de entrada\"\"\"\n",
    "        \n",
    "        print(\"1. Carregando arquivos...\")\n",
    "        \n",
    "        # Carregar dados\n",
    "        self.b1_df = pd.read_csv(trilha_b1_path)  # Capacidade histórica\n",
    "        self.b2_df = pd.read_csv(trilha_b2_path)  # Histórico por item\n",
    "        self.b3_df = pd.read_csv(trilha_b3_path)  # Capacidade por dev\n",
    "        self.b4_df = pd.read_csv(trilha_b4_path)  # Backlog atual\n",
    "        self.priorizado_df = pd.read_csv(backlog_priorizado_path)  # Saída Trilha A\n",
    "        \n",
    "        # Normalizar colunas\n",
    "        self._normalize_columns()\n",
    "        \n",
    "        print(\"2. Preparando dados...\")\n",
    "        \n",
    "        # Preparar backlog principal\n",
    "        self._prepare_backlog()\n",
    "        \n",
    "        # Construir grafo de dependências\n",
    "        self._build_dependency_graph()\n",
    "        \n",
    "        print(\"3. Treinando modelos preditivos...\")\n",
    "        \n",
    "        # Treinar modelos\n",
    "        self._train_capacity_model()\n",
    "        self._train_overrun_model()\n",
    "        self._train_spillover_model()\n",
    "        \n",
    "        print(\"✅ Dados carregados e modelos treinados!\")\n",
    "    \n",
    "    def _normalize_columns(self) -> None:\n",
    "        \"\"\"Normaliza nomes das colunas\"\"\"\n",
    "        # Corrigir coluna problemática em B3\n",
    "        if 'horas disponiveis ' in self.b3_df.columns:\n",
    "            self.b3_df = self.b3_df.rename(columns={'horas disponiveis ': 'horas_disponiveis'})\n",
    "        \n",
    "        # Padronizar nomes\n",
    "        for df in [self.b1_df, self.b2_df, self.b3_df, self.b4_df, self.priorizado_df]:\n",
    "            df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "    \n",
    "    def _prepare_backlog(self) -> None:\n",
    "        \"\"\"Prepara o backlog principal combinando dados das trilhas\"\"\"\n",
    "        # Começar com B4 (backlog atual)\n",
    "        self.backlog_df = self.b4_df.copy()\n",
    "        \n",
    "        # Garantir que issueid seja string para joins\n",
    "        self.backlog_df['issueid'] = self.backlog_df['issueid'].astype(str)\n",
    "        self.priorizado_df['id'] = self.priorizado_df['id'].astype(str)\n",
    "        \n",
    "        # Juntar com dados de valor (Trilha A)\n",
    "        valor_cols = ['id', 'roi_ajustado', 'score_ml', 'dep_impact']\n",
    "        if all(col in self.priorizado_df.columns for col in valor_cols):\n",
    "            valor_df = self.priorizado_df[valor_cols].copy()\n",
    "            self.backlog_df = self.backlog_df.merge(\n",
    "                valor_df, \n",
    "                left_on='issueid', \n",
    "                right_on='id', \n",
    "                how='left'\n",
    "            )\n",
    "            # Usar score_ml calibrado como valor principal\n",
    "            self.backlog_df['valor_trilha_a'] = self.backlog_df['score_ml'].fillna(\n",
    "                self.backlog_df['roi_ajustado'].fillna(\n",
    "                    self.backlog_df['prioridade_score']\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            # Fallback: usar prioridade_score\n",
    "            self.backlog_df['valor_trilha_a'] = self.backlog_df['prioridade_score']\n",
    "        \n",
    "        # Parsear dependências\n",
    "        self.backlog_df['deps_parsed'] = self.backlog_df['dependencias'].apply(self._parse_dependencies)\n",
    "        \n",
    "        print(f\"   Backlog preparado: {len(self.backlog_df)} itens\")\n",
    "    \n",
    "    def _parse_dependencies(self, deps_str: Any) -> List[str]:\n",
    "        \"\"\"Parseia string de dependências em lista\"\"\"\n",
    "        if pd.isna(deps_str) or deps_str == '':\n",
    "            return []\n",
    "        \n",
    "        deps_str = str(deps_str).strip()\n",
    "        if not deps_str:\n",
    "            return []\n",
    "        \n",
    "        # Tentar parsear como JSON\n",
    "        try:\n",
    "            if deps_str.startswith('[') and deps_str.endswith(']'):\n",
    "                return json.loads(deps_str)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Parsear com separadores\n",
    "        deps = []\n",
    "        for sep in [',', ';']:\n",
    "            if sep in deps_str:\n",
    "                deps = [dep.strip().strip('\"\\'') for dep in deps_str.split(sep)]\n",
    "                break\n",
    "        else:\n",
    "            deps = [deps_str.strip().strip('\"\\'')]\n",
    "        \n",
    "        return [str(dep) for dep in deps if dep]\n",
    "    \n",
    "    def _build_dependency_graph(self) -> None:\n",
    "        \"\"\"Constrói grafo de dependências\"\"\"\n",
    "        self.dependency_graph = nx.DiGraph()\n",
    "        \n",
    "        # Todos os IDs válidos\n",
    "        valid_ids = set(self.backlog_df['issueid'].astype(str))\n",
    "        self.dependency_graph.add_nodes_from(valid_ids)\n",
    "        \n",
    "        # Adicionar arestas e detectar dependências externas\n",
    "        external_deps = defaultdict(list)\n",
    "        \n",
    "        for _, row in self.backlog_df.iterrows():\n",
    "            item_id = str(row['issueid'])\n",
    "            \n",
    "            for dep in row['deps_parsed']:\n",
    "                dep = str(dep)\n",
    "                if dep in valid_ids:\n",
    "                    # Aresta: dependência → item\n",
    "                    self.dependency_graph.add_edge(dep, item_id)\n",
    "                else:\n",
    "                    external_deps[item_id].append(dep)\n",
    "        \n",
    "        # Marcar dependências externas\n",
    "        self.backlog_df['has_external_deps'] = self.backlog_df['issueid'].astype(str).map(\n",
    "            lambda x: len(external_deps[x]) > 0\n",
    "        )\n",
    "        \n",
    "        # Detectar e quebrar ciclos se necessário\n",
    "        if not nx.is_directed_acyclic_graph(self.dependency_graph):\n",
    "            self._break_cycles()\n",
    "        \n",
    "        print(f\"   Grafo: {len(self.dependency_graph.nodes())} nós, {len(self.dependency_graph.edges())} arestas\")\n",
    "    \n",
    "    def _break_cycles(self) -> None:\n",
    "        \"\"\"Quebra ciclos no grafo\"\"\"\n",
    "        valor_dict = dict(zip(\n",
    "            self.backlog_df['issueid'].astype(str), \n",
    "            self.backlog_df['valor_trilha_a']\n",
    "        ))\n",
    "        \n",
    "        while not nx.is_directed_acyclic_graph(self.dependency_graph):\n",
    "            try:\n",
    "                cycle = nx.find_cycle(self.dependency_graph)\n",
    "                min_weight = float('inf')\n",
    "                edge_to_remove = None\n",
    "                \n",
    "                for u, v in cycle:\n",
    "                    weight = valor_dict.get(u, 0) + valor_dict.get(v, 0)\n",
    "                    if weight < min_weight:\n",
    "                        min_weight = weight\n",
    "                        edge_to_remove = (u, v)\n",
    "                \n",
    "                if edge_to_remove:\n",
    "                    self.dependency_graph.remove_edge(*edge_to_remove)\n",
    "                    print(f\"   Ciclo quebrado: removida aresta {edge_to_remove}\")\n",
    "                else:\n",
    "                    break\n",
    "            except nx.NetworkXNoCycle:\n",
    "                break\n",
    "    \n",
    "    def _train_capacity_model(self) -> None:\n",
    "        \"\"\"Treina modelo de previsão de capacidade\"\"\"\n",
    "        # Filtrar por team\n",
    "        team_data = self.b1_df[self.b1_df['teamid'] == self.team_id].copy()\n",
    "        \n",
    "        if len(team_data) < 3:\n",
    "            print(f\"   Poucos dados para team {self.team_id}, usando média histórica\")\n",
    "            mean_capacity = team_data['sp_entregues'].mean() if len(team_data) > 0 else 10\n",
    "            std_capacity = team_data['sp_entregues'].std() if len(team_data) > 1 else 2\n",
    "            self.capacity_distribution = {\n",
    "                'mean': mean_capacity,\n",
    "                'std': max(std_capacity, 1.0),\n",
    "                'type': 'normal'\n",
    "            }\n",
    "            return\n",
    "        \n",
    "        # Ordenar por sprint\n",
    "        team_data = team_data.sort_values('sprintid')\n",
    "        ts = team_data['sp_entregues'].values\n",
    "        \n",
    "        try:\n",
    "            # Tentar ETS\n",
    "            model = ExponentialSmoothing(ts, trend='add', seasonal=None)\n",
    "            fitted_model = model.fit()\n",
    "            \n",
    "            # Prever próximo valor\n",
    "            forecast = fitted_model.forecast(steps=1)\n",
    "            residuals = fitted_model.resid\n",
    "            \n",
    "            self.capacity_distribution = {\n",
    "                'mean': float(forecast[0]),\n",
    "                'std': float(np.std(residuals)) if len(residuals) > 1 else 2.0,\n",
    "                'type': 'normal'\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Fallback: usar média e desvio históricos\n",
    "            self.capacity_distribution = {\n",
    "                'mean': float(np.mean(ts)),\n",
    "                'std': float(np.std(ts)) if len(ts) > 1 else 2.0,\n",
    "                'type': 'normal'\n",
    "            }\n",
    "        \n",
    "        print(f\"   Capacidade: μ={self.capacity_distribution['mean']:.1f}, σ={self.capacity_distribution['std']:.1f}\")\n",
    "    \n",
    "    def _train_overrun_model(self) -> None:\n",
    "        \"\"\"Treina modelo de overrun (razão realizado/estimado)\"\"\"\n",
    "        # Preparar dados de overrun\n",
    "        self.b2_df['overrun_ratio'] = self.b2_df['storypoints_realizados'] / self.b2_df['storypoints_estimados']\n",
    "        self.b2_df['log_overrun'] = np.log(self.b2_df['overrun_ratio'])\n",
    "        \n",
    "        # Features disponíveis\n",
    "        features = []\n",
    "        X_data = pd.DataFrame()\n",
    "        \n",
    "        # SP estimados (binned)\n",
    "        X_data['sp_estimados'] = self.b2_df['storypoints_estimados']\n",
    "        X_data['sp_bin'] = pd.cut(X_data['sp_estimados'], bins=[0, 2, 5, 8, float('inf')], labels=[0, 1, 2, 3])\n",
    "        features.extend(['sp_estimados', 'sp_bin'])\n",
    "        \n",
    "        # Spillover como feature\n",
    "        if 'spillover_(dep_externa)' in self.b2_df.columns:\n",
    "            X_data['spillover'] = self.b2_df['spillover_(dep_externa)']\n",
    "            features.append('spillover')\n",
    "        \n",
    "        # Alvo\n",
    "        y = self.b2_df['log_overrun'].fillna(0)\n",
    "        \n",
    "        if len(X_data) > 10:\n",
    "            try:\n",
    "                # Treinar LightGBM\n",
    "                self.overrun_model = lgb.LGBMRegressor(\n",
    "                    n_estimators=100,\n",
    "                    random_state=self.random_state,\n",
    "                    verbose=-1\n",
    "                )\n",
    "                self.overrun_model.fit(X_data[features], y)\n",
    "                \n",
    "                # Calcular resíduos para incerteza\n",
    "                predictions = self.overrun_model.predict(X_data[features])\n",
    "                residuals = y - predictions\n",
    "                residual_std = np.std(residuals)\n",
    "                \n",
    "                print(f\"   Modelo overrun treinado (MAE: {mean_absolute_error(y, predictions):.3f})\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   Erro no modelo overrun, usando médias: {e}\")\n",
    "                self.overrun_model = None\n",
    "                residual_std = np.std(y)\n",
    "        else:\n",
    "            self.overrun_model = None\n",
    "            residual_std = np.std(y) if len(y) > 1 else 0.2\n",
    "        \n",
    "        # Distribuições por faixas de SP\n",
    "        for sp_range in [(0, 2), (2, 5), (5, 8), (8, float('inf'))]:\n",
    "            mask = (self.b2_df['storypoints_estimados'] >= sp_range[0]) & \\\n",
    "                   (self.b2_df['storypoints_estimados'] < sp_range[1])\n",
    "            \n",
    "            if mask.sum() > 0:\n",
    "                range_data = self.b2_df[mask]['log_overrun']\n",
    "                self.overrun_distributions[sp_range] = {\n",
    "                    'mean': float(range_data.mean()),\n",
    "                    'std': max(float(range_data.std()), residual_std, 0.1)\n",
    "                }\n",
    "            else:\n",
    "                self.overrun_distributions[sp_range] = {\n",
    "                    'mean': 0.0,\n",
    "                    'std': residual_std\n",
    "                }\n",
    "    \n",
    "    def _train_spillover_model(self) -> None:\n",
    "        \"\"\"Treina modelo de probabilidade de spillover\"\"\"\n",
    "        if 'spillover_(dep_externa)' not in self.b2_df.columns:\n",
    "            # Criar probabilidades base por faixa de SP\n",
    "            for sp_range in [(0, 2), (2, 5), (5, 8), (8, float('inf'))]:\n",
    "                self.spillover_probabilities[sp_range] = 0.1  # 10% base\n",
    "            return\n",
    "        \n",
    "        # Agrupar por faixas de SP e presença de dep externa\n",
    "        self.b2_df['sp_bin'] = pd.cut(\n",
    "            self.b2_df['storypoints_estimados'], \n",
    "            bins=[0, 2, 5, 8, float('inf')],\n",
    "            labels=['small', 'medium', 'large', 'xlarge']\n",
    "        )\n",
    "        \n",
    "        # Calcular probabilidades por faixa\n",
    "        spillover_rates = self.b2_df.groupby('sp_bin')['spillover_(dep_externa)'].mean()\n",
    "        \n",
    "        for i, sp_range in enumerate([(0, 2), (2, 5), (5, 8), (8, float('inf'))]):\n",
    "            bin_label = ['small', 'medium', 'large', 'xlarge'][i]\n",
    "            if bin_label in spillover_rates.index:\n",
    "                self.spillover_probabilities[sp_range] = max(0.05, min(0.5, spillover_rates[bin_label]))\n",
    "            else:\n",
    "                self.spillover_probabilities[sp_range] = 0.1\n",
    "        \n",
    "        print(f\"   Spillover rates: {dict(zip(['S', 'M', 'L', 'XL'], self.spillover_probabilities.values()))}\")\n",
    "    \n",
    "    def _get_sp_range(self, sp: float) -> Tuple[float, float]:\n",
    "        \"\"\"Retorna a faixa de SP para um valor\"\"\"\n",
    "        for sp_range in [(0, 2), (2, 5), (5, 8), (8, float('inf'))]:\n",
    "            if sp >= sp_range[0] and sp < sp_range[1]:\n",
    "                return sp_range\n",
    "        return (8, float('inf'))\n",
    "    \n",
    "    def monte_carlo_simulation(self) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Executa simulação Monte Carlo\"\"\"\n",
    "        print(f\"4. Executando {self.n_simulations} simulações Monte Carlo...\")\n",
    "        \n",
    "        results = {\n",
    "            'capacity_samples': np.zeros(self.n_simulations),\n",
    "            'overrun_samples': {},  # Por item\n",
    "            'sp_efetivo_samples': {}  # Por item\n",
    "        }\n",
    "        \n",
    "        # Amostrar capacidades\n",
    "        results['capacity_samples'] = np.random.normal(\n",
    "            self.capacity_distribution['mean'],\n",
    "            self.capacity_distribution['std'],\n",
    "            self.n_simulations\n",
    "        )\n",
    "        results['capacity_samples'] = np.maximum(results['capacity_samples'], 1)  # Mínimo 1 SP\n",
    "        \n",
    "        # Amostrar overrun por item\n",
    "        for _, item in self.backlog_df.iterrows():\n",
    "            item_id = str(item['issueid'])\n",
    "            sp = item['storypoints']\n",
    "            sp_range = self._get_sp_range(sp)\n",
    "            \n",
    "            if self.overrun_model is not None:\n",
    "                # Usar modelo se disponível\n",
    "                try:\n",
    "                    item_features = pd.DataFrame({\n",
    "                        'sp_estimados': [sp],\n",
    "                        'sp_bin': [self._get_sp_bin(sp)],\n",
    "                        'spillover': [int(item.get('has_external_deps', False))]\n",
    "                    })\n",
    "                    \n",
    "                    log_overrun_pred = self.overrun_model.predict(item_features)[0]\n",
    "                    log_overrun_std = self.overrun_distributions[sp_range]['std']\n",
    "                    \n",
    "                    log_overrun_samples = np.random.normal(\n",
    "                        log_overrun_pred, \n",
    "                        log_overrun_std, \n",
    "                        self.n_simulations\n",
    "                    )\n",
    "                except:\n",
    "                    # Fallback para distribuição por faixa\n",
    "                    dist = self.overrun_distributions[sp_range]\n",
    "                    log_overrun_samples = np.random.normal(\n",
    "                        dist['mean'], \n",
    "                        dist['std'], \n",
    "                        self.n_simulations\n",
    "                    )\n",
    "            else:\n",
    "                # Usar distribuição por faixa de SP\n",
    "                dist = self.overrun_distributions[sp_range]\n",
    "                log_overrun_samples = np.random.normal(\n",
    "                    dist['mean'], \n",
    "                    dist['std'], \n",
    "                    self.n_simulations\n",
    "                )\n",
    "            \n",
    "            # Converter para overrun ratio\n",
    "            overrun_samples = np.exp(log_overrun_samples)\n",
    "            overrun_samples = np.maximum(overrun_samples, 0.5)  # Mínimo 50% do estimado\n",
    "            overrun_samples = np.minimum(overrun_samples, 3.0)  # Máximo 300% do estimado\n",
    "            \n",
    "            results['overrun_samples'][item_id] = overrun_samples\n",
    "            results['sp_efetivo_samples'][item_id] = sp * overrun_samples\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _get_sp_bin(self, sp: float) -> int:\n",
    "        \"\"\"Converte SP em bin categórico\"\"\"\n",
    "        if sp <= 2:\n",
    "            return 0\n",
    "        elif sp <= 5:\n",
    "            return 1\n",
    "        elif sp <= 8:\n",
    "            return 2\n",
    "        else:\n",
    "            return 3\n",
    "    \n",
    "    def optimize_sprint(self, simulation_results: Dict) -> Tuple[List[str], Dict]:\n",
    "        \"\"\"Otimiza seleção de itens para o sprint\"\"\"\n",
    "        print(\"5. Otimizando seleção de itens...\")\n",
    "        \n",
    "        # Algoritmo guloso com chance constraints\n",
    "        best_sprint = []\n",
    "        best_value = 0\n",
    "        best_prob_on_time = 0\n",
    "        \n",
    "        # Criar lista de candidatos ordenados por ganho marginal esperado\n",
    "        candidates = []\n",
    "        \n",
    "        for _, item in self.backlog_df.iterrows():\n",
    "            item_id = str(item['issueid'])\n",
    "            sp_range = self._get_sp_range(item['storypoints'])\n",
    "            spillover_prob = self.spillover_probabilities.get(sp_range, 0.1)\n",
    "            \n",
    "            # Penalização por spillover\n",
    "            spillover_penalty = 1.0 - self.lambda_spillover * spillover_prob\n",
    "            if item.get('has_external_deps', False):\n",
    "                spillover_penalty *= 0.8  # Penalização adicional por dep externa\n",
    "            \n",
    "            # Ganho marginal esperado\n",
    "            expected_sp_efetivo = np.mean(simulation_results['sp_efetivo_samples'][item_id])\n",
    "            marginal_gain = (item['valor_trilha_a'] * spillover_penalty) / expected_sp_efetivo\n",
    "            \n",
    "            candidates.append({\n",
    "                'id': item_id,\n",
    "                'value': item['valor_trilha_a'],\n",
    "                'marginal_gain': marginal_gain,\n",
    "                'sp_estimado': item['storypoints'],\n",
    "                'deps': item['deps_parsed']\n",
    "            })\n",
    "        \n",
    "        # Ordenar por ganho marginal\n",
    "        candidates.sort(key=lambda x: x['marginal_gain'], reverse=True)\n",
    "        \n",
    "        # Seleção gulosa respeitando dependências\n",
    "        current_sprint = self._greedy_selection_with_dependencies(\n",
    "            candidates, simulation_results\n",
    "        )\n",
    "        \n",
    "        # Calcular probabilidade de sucesso\n",
    "        prob_on_time = self._calculate_success_probability(current_sprint, simulation_results)\n",
    "        \n",
    "        # Ajustar para atingir meta theta\n",
    "        if prob_on_time < self.theta:\n",
    "            current_sprint = self._adjust_for_reliability(\n",
    "                current_sprint, simulation_results\n",
    "            )\n",
    "            prob_on_time = self._calculate_success_probability(current_sprint, simulation_results)\n",
    "        \n",
    "        # Métricas finais\n",
    "        total_value = sum(self.backlog_df[self.backlog_df['issueid'].astype(str).isin(current_sprint)]['valor_trilha_a'])\n",
    "        total_sp_estimado = sum(self.backlog_df[self.backlog_df['issueid'].astype(str).isin(current_sprint)]['storypoints'])\n",
    "        \n",
    "        expected_sp_efetivo = sum(\n",
    "            np.mean(simulation_results['sp_efetivo_samples'][item_id])\n",
    "            for item_id in current_sprint\n",
    "        )\n",
    "        \n",
    "        metrics = {\n",
    "            'prob_on_time': prob_on_time,\n",
    "            'total_value': total_value,\n",
    "            'total_sp_estimado': total_sp_estimado,\n",
    "            'expected_sp_efetivo': expected_sp_efetivo,\n",
    "            'n_items': len(current_sprint),\n",
    "            'capacity_mean': self.capacity_distribution['mean']\n",
    "        }\n",
    "        \n",
    "        return current_sprint, metrics\n",
    "    \n",
    "    def _greedy_selection_with_dependencies(self, candidates: List[Dict], \n",
    "                                           simulation_results: Dict) -> List[str]:\n",
    "        \"\"\"Seleção gulosa respeitando dependências\"\"\"\n",
    "        selected = []\n",
    "        G = self.dependency_graph.copy()\n",
    "        \n",
    "        while candidates:\n",
    "            # Encontrar candidatos sem dependências não satisfeitas\n",
    "            available = []\n",
    "            for candidate in candidates:\n",
    "                item_id = candidate['id']\n",
    "                # Verificar se todas as dependências já estão selecionadas\n",
    "                deps_satisfied = all(\n",
    "                    dep in selected or dep not in G.nodes()\n",
    "                    for dep in candidate['deps']\n",
    "                )\n",
    "                if deps_satisfied:\n",
    "                    available.append(candidate)\n",
    "            \n",
    "            if not available:\n",
    "                break\n",
    "            \n",
    "            # Escolher melhor disponível\n",
    "            best_candidate = max(available, key=lambda x: x['marginal_gain'])\n",
    "            item_id = best_candidate['id']\n",
    "            \n",
    "            # Verificar se cabe no sprint (usando percentil 80 da capacidade)\n",
    "            capacity_p80 = np.percentile(simulation_results['capacity_samples'], 80)\n",
    "            current_sp_efetivo = sum(\n",
    "                np.percentile(simulation_results['sp_efetivo_samples'][sel_id], 80)\n",
    "                for sel_id in selected\n",
    "            )\n",
    "            item_sp_efetivo = np.percentile(simulation_results['sp_efetivo_samples'][item_id], 80)\n",
    "            \n",
    "            if current_sp_efetivo + item_sp_efetivo <= capacity_p80:\n",
    "                selected.append(item_id)\n",
    "            \n",
    "            # Remover candidato da lista\n",
    "            candidates.remove(best_candidate)\n",
    "        \n",
    "        return selected\n",
    "    \n",
    "    def _calculate_success_probability(self, sprint_items: List[str], \n",
    "                                     simulation_results: Dict) -> float:\n",
    "        \"\"\"Calcula P(on-time) para um sprint\"\"\"\n",
    "        if not sprint_items:\n",
    "            return 1.0\n",
    "        \n",
    "        successes = 0\n",
    "        for i in range(self.n_simulations):\n",
    "            capacity = simulation_results['capacity_samples'][i]\n",
    "            total_sp_efetivo = sum(\n",
    "                simulation_results['sp_efetivo_samples'][item_id][i]\n",
    "                for item_id in sprint_items\n",
    "            )\n",
    "            \n",
    "            if total_sp_efetivo <= capacity:\n",
    "                successes += 1\n",
    "        \n",
    "        return successes / self.n_simulations\n",
    "    \n",
    "    def _adjust_for_reliability(self, initial_sprint: List[str], \n",
    "                               simulation_results: Dict) -> List[str]:\n",
    "        \"\"\"Ajusta sprint para atingir meta de confiabilidade\"\"\"\n",
    "        current_sprint = initial_sprint.copy()\n",
    "        \n",
    "        while (self._calculate_success_probability(current_sprint, simulation_results) < self.theta \n",
    "               and len(current_sprint) > 0):\n",
    "            \n",
    "            # Remover item com menor ganho marginal\n",
    "            worst_item = None\n",
    "            worst_gain = float('inf')\n",
    "            \n",
    "            for item_id in current_sprint:\n",
    "                item_data = self.backlog_df[self.backlog_df['issueid'].astype(str) == item_id].iloc[0]\n",
    "                expected_sp = np.mean(simulation_results['sp_efetivo_samples'][item_id])\n",
    "                gain = item_data['valor_trilha_a'] / expected_sp\n",
    "                \n",
    "                if gain < worst_gain:\n",
    "                    worst_gain = gain\n",
    "                    worst_item = item_id\n",
    "            \n",
    "            if worst_item:\n",
    "                current_sprint.remove(worst_item)\n",
    "        \n",
    "        return current_sprint\n",
    "    \n",
    "    def generate_execution_order(self, sprint_items: List[str]) -> List[str]:\n",
    "        \"\"\"Gera ordem de execução respeitando dependências\"\"\"\n",
    "        if not sprint_items:\n",
    "            return []\n",
    "        \n",
    "        # Subgrafo apenas com itens do sprint\n",
    "        subgraph = self.dependency_graph.subgraph(sprint_items).copy()\n",
    "        \n",
    "        # Ordenação topológica com priorização por valor\n",
    "        ordered = []\n",
    "        while subgraph.nodes():\n",
    "            # Encontrar nós sem dependências\n",
    "            available = [node for node in subgraph.nodes() if subgraph.in_degree(node) == 0]\n",
    "            \n",
    "            if not available:\n",
    "                # Se há ciclos, pegar qualquer nó\n",
    "                available = list(subgraph.nodes())\n",
    "            \n",
    "            # Escolher por maior valor\n",
    "            best_item = max(available, key=lambda x: \n",
    "                self.backlog_df[self.backlog_df['issueid'].astype(str) == x]['valor_trilha_a'].iloc[0]\n",
    "            )\n",
    "            \n",
    "            ordered.append(best_item)\n",
    "            subgraph.remove_node(best_item)\n",
    "        \n",
    "        return ordered\n",
    "    \n",
    "    def plan_sprint(self, \n",
    "                   trilha_b1_path: str = \"/workspaces/codespaces-jupyter/data/Backlog - trilhaB1.csv\",\n",
    "                   trilha_b2_path: str = \"/workspaces/codespaces-jupyter/data/Backlog - trilhaB2.csv\",\n",
    "                   trilha_b3_path: str = \"/workspaces/codespaces-jupyter/data/Backlog - trilhaB3.csv\", \n",
    "                   trilha_b4_path: str = \"/workspaces/codespaces-jupyter/data/Backlog - trilhaB4.csv\",\n",
    "                   backlog_priorizado_path: str = \"/workspaces/codespaces-jupyter/data/backlog_priorizado.csv\") -> Dict:\n",
    "        \"\"\"Executa todo o pipeline de planejamento\"\"\"\n",
    "        \n",
    "        # Carregar dados\n",
    "        self.load_and_process_data(\n",
    "            trilha_b1_path, trilha_b2_path, trilha_b3_path,\n",
    "            trilha_b4_path, backlog_priorizado_path\n",
    "        )\n",
    "        \n",
    "        # Simulação Monte Carlo\n",
    "        simulation_results = self.monte_carlo_simulation()\n",
    "        \n",
    "        # Otimização\n",
    "        sprint_items, metrics = self.optimize_sprint(simulation_results)\n",
    "        \n",
    "        # Ordem de execução\n",
    "        execution_order = self.generate_execution_order(sprint_items)\n",
    "        \n",
    "        # Compilar resultado final\n",
    "        result = {\n",
    "            'sprint_items': sprint_items,\n",
    "            'execution_order': execution_order,\n",
    "            'metrics': metrics,\n",
    "            'items_details': self.backlog_df[\n",
    "                self.backlog_df['issueid'].astype(str).isin(sprint_items)\n",
    "            ][['issueid', 'valor_trilha_a', 'storypoints', 'deps_parsed']].to_dict('records')\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def print_results(self, result: Dict) -> None:\n",
    "        \"\"\"Imprime resultados de forma organizada\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"📊 PLANEJAMENTO DE SPRINT - TEAM {self.team_id}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        metrics = result['metrics']\n",
    "        \n",
    "        # Métricas principais\n",
    "        print(f\"\\n🎯 MÉTRICAS PRINCIPAIS:\")\n",
    "        print(f\"   Probabilidade de sucesso: {metrics['prob_on_time']:.1%} (meta: {self.theta:.1%})\")\n",
    "        print(f\"   Valor total do sprint: {metrics['total_value']:.2f}\")\n",
    "        print(f\"   Story Points estimados: {metrics['total_sp_estimado']}\")\n",
    "        print(f\"   Story Points efetivos (esperado): {metrics['expected_sp_efetivo']:.1f}\")\n",
    "        print(f\"   Capacidade média do time: {metrics['capacity_mean']:.1f} SP\")\n",
    "        print(f\"   Número de itens: {metrics['n_items']}\")\n",
    "        \n",
    "        # Indicadores de risco\n",
    "        utilization = metrics['expected_sp_efetivo'] / metrics['capacity_mean']\n",
    "        risk_level = \"🟢 BAIXO\" if utilization < 0.7 else \"🟡 MÉDIO\" if utilization < 0.9 else \"🔴 ALTO\"\n",
    "        print(f\"   Utilização da capacidade: {utilization:.1%} - {risk_level}\")\n",
    "        \n",
    "        # Lista de itens selecionados\n",
    "        print(f\"\\n📋 ITENS SELECIONADOS (Ordem de Execução):\")\n",
    "        for i, item_id in enumerate(result['execution_order'], 1):\n",
    "            item_detail = next(\n",
    "                (item for item in result['items_details'] if str(item['issueid']) == item_id), \n",
    "                None\n",
    "            )\n",
    "            if item_detail:\n",
    "                deps_str = f\" (deps: {item_detail['deps_parsed']})\" if item_detail['deps_parsed'] else \"\"\n",
    "                print(f\"   {i:2d}. ID {item_id} | Valor: {item_detail['valor_trilha_a']:.2f} | SP: {item_detail['storypoints']}{deps_str}\")\n",
    "        \n",
    "        # Recomendações\n",
    "        print(f\"\\n💡 RECOMENDAÇÕES:\")\n",
    "        if metrics['prob_on_time'] < self.theta:\n",
    "            print(f\"   ⚠️  Probabilidade de sucesso abaixo da meta ({self.theta:.1%})\")\n",
    "            print(f\"   📉 Considere remover itens ou revisar estimativas\")\n",
    "        else:\n",
    "            print(f\"   ✅ Sprint atende à meta de confiabilidade\")\n",
    "        \n",
    "        if utilization > 0.85:\n",
    "            print(f\"   ⚠️  Alta utilização da capacidade - risco de spillover\")\n",
    "        \n",
    "        # Itens de fronteira (não selecionados mas com alta prioridade)\n",
    "        print(f\"\\n🔄 ITENS DE FRONTEIRA (próximos da seleção):\")\n",
    "        selected_ids = set(result['sprint_items'])\n",
    "        frontier_items = self.backlog_df[\n",
    "            ~self.backlog_df['issueid'].astype(str).isin(selected_ids)\n",
    "        ].nlargest(3, 'valor_trilha_a')\n",
    "        \n",
    "        for _, item in frontier_items.iterrows():\n",
    "            print(f\"   • ID {item['issueid']} | Valor: {item['valor_trilha_a']:.2f} | SP: {item['storypoints']}\")\n",
    "\n",
    "    def save_results(self, result: Dict, output_path: str) -> None:\n",
    "        \"\"\"Salva resultados em CSV\"\"\"\n",
    "        # Preparar dados para salvar\n",
    "        output_data = []\n",
    "        \n",
    "        for i, item_id in enumerate(result['execution_order'], 1):\n",
    "            item_detail = next(\n",
    "                (item for item in result['items_details'] if str(item['issueid']) == item_id),\n",
    "                None\n",
    "            )\n",
    "            \n",
    "            if item_detail:\n",
    "                output_data.append({\n",
    "                    'execution_order': i,\n",
    "                    'item_id': item_id,\n",
    "                    'valor_trilha_a': item_detail['valor_trilha_a'],\n",
    "                    'story_points': item_detail['storypoints'],\n",
    "                    'dependencies': str(item_detail['deps_parsed']),\n",
    "                    'has_dependencies': len(item_detail['deps_parsed']) > 0\n",
    "                })\n",
    "        \n",
    "        # Adicionar métricas como linhas extras\n",
    "        metrics = result['metrics']\n",
    "        \n",
    "        # Salvar\n",
    "        df_output = pd.DataFrame(output_data)\n",
    "        df_output.to_csv(output_path, index=False)\n",
    "        \n",
    "        # Salvar métricas em arquivo separado\n",
    "        metrics_path = output_path.replace('.csv', '_metrics.txt')\n",
    "        with open(metrics_path, 'w') as f:\n",
    "            f.write(f\"PLANEJAMENTO DE SPRINT - TEAM {self.team_id}\\n\")\n",
    "            f.write(\"=\"*50 + \"\\n\\n\")\n",
    "            f.write(f\"Probabilidade de sucesso: {metrics['prob_on_time']:.1%}\\n\")\n",
    "            f.write(f\"Valor total: {metrics['total_value']:.2f}\\n\")\n",
    "            f.write(f\"SP estimados: {metrics['total_sp_estimado']}\\n\")\n",
    "            f.write(f\"SP efetivos esperados: {metrics['expected_sp_efetivo']:.1f}\\n\")\n",
    "            f.write(f\"Capacidade média: {metrics['capacity_mean']:.1f}\\n\")\n",
    "            f.write(f\"Número de itens: {metrics['n_items']}\\n\")\n",
    "            f.write(f\"Utilização: {metrics['expected_sp_efetivo']/metrics['capacity_mean']:.1%}\\n\")\n",
    "        \n",
    "        print(f\"✅ Resultados salvos em: {output_path}\")\n",
    "        print(f\"✅ Métricas salvas em: {metrics_path}\")\n",
    "\n",
    "# Função principal para execução\n",
    "def main():\n",
    "    \"\"\"Exemplo de execução do planejamento de sprint\"\"\"\n",
    "    \n",
    "    # Parâmetros do planejador\n",
    "    planner = SprintPlanner(\n",
    "        team_id=1,\n",
    "        theta=0.8,  # 80% de confiabilidade\n",
    "        n_simulations=1000,  # Reduzido para exemplo rápido\n",
    "        lambda_spillover=0.1,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        print(\"🚀 Iniciando planejamento de sprint...\")\n",
    "        \n",
    "        # Executar planejamento completo\n",
    "        result = planner.plan_sprint()\n",
    "        \n",
    "        # Mostrar resultados\n",
    "        planner.print_results(result)\n",
    "        \n",
    "        # Salvar resultados\n",
    "        planner.save_results(result, '/workspaces/codespaces-jupyter/data/sprint_planejado.csv')\n",
    "        \n",
    "        print(\"\\n✅ Planejamento concluído com sucesso!\")\n",
    "        \n",
    "        # Análise adicional\n",
    "        print(f\"\\n🔍 ANÁLISE ADICIONAL:\")\n",
    "        \n",
    "        # Verificar se há itens bloqueados\n",
    "        blocked_items = planner.backlog_df[\n",
    "            planner.backlog_df['deps_parsed'].apply(len) > 0\n",
    "        ]\n",
    "        if len(blocked_items) > 0:\n",
    "            print(f\"   📌 {len(blocked_items)} itens com dependências no backlog\")\n",
    "        \n",
    "        # Distribuição de tamanhos\n",
    "        size_dist = planner.backlog_df['storypoints'].value_counts().sort_index()\n",
    "        print(f\"   📊 Distribuição de tamanhos: {dict(size_dist)}\")\n",
    "        \n",
    "        # Capacidade vs demanda\n",
    "        total_backlog_sp = planner.backlog_df['storypoints'].sum()\n",
    "        sprints_needed = total_backlog_sp / planner.capacity_distribution['mean']\n",
    "        print(f\"   ⏱️  Sprints estimados para todo o backlog: {sprints_needed:.1f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erro durante planejamento: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Classe utilitária para análise de sensibilidade\n",
    "class SensitivityAnalyzer:\n",
    "    \"\"\"Análise de sensibilidade dos parâmetros do planejamento\"\"\"\n",
    "    \n",
    "    def __init__(self, planner: SprintPlanner):\n",
    "        self.planner = planner\n",
    "    \n",
    "    def analyze_theta_sensitivity(self, theta_range: List[float]) -> pd.DataFrame:\n",
    "        \"\"\"Analisa como diferentes valores de theta afetam o sprint\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        original_theta = self.planner.theta\n",
    "        simulation_results = self.planner.monte_carlo_simulation()\n",
    "        \n",
    "        for theta in theta_range:\n",
    "            self.planner.theta = theta\n",
    "            sprint_items, metrics = self.planner.optimize_sprint(simulation_results)\n",
    "            \n",
    "            results.append({\n",
    "                'theta': theta,\n",
    "                'n_items': metrics['n_items'],\n",
    "                'total_value': metrics['total_value'],\n",
    "                'prob_on_time': metrics['prob_on_time'],\n",
    "                'sp_estimado': metrics['total_sp_estimado'],\n",
    "                'sp_efetivo': metrics['expected_sp_efetivo']\n",
    "            })\n",
    "        \n",
    "        # Restaurar theta original\n",
    "        self.planner.theta = original_theta\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "    \n",
    "    def analyze_capacity_impact(self, capacity_adjustments: List[float]) -> pd.DataFrame:\n",
    "        \"\"\"Analisa impacto de diferentes cenários de capacidade\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        original_capacity = self.planner.capacity_distribution.copy()\n",
    "        \n",
    "        for adjustment in capacity_adjustments:\n",
    "            # Ajustar capacidade\n",
    "            self.planner.capacity_distribution['mean'] = original_capacity['mean'] * adjustment\n",
    "            \n",
    "            # Re-executar simulação e otimização\n",
    "            simulation_results = self.planner.monte_carlo_simulation()\n",
    "            sprint_items, metrics = self.planner.optimize_sprint(simulation_results)\n",
    "            \n",
    "            results.append({\n",
    "                'capacity_factor': adjustment,\n",
    "                'capacity_mean': self.planner.capacity_distribution['mean'],\n",
    "                'n_items': metrics['n_items'],\n",
    "                'total_value': metrics['total_value'],\n",
    "                'prob_on_time': metrics['prob_on_time']\n",
    "            })\n",
    "        \n",
    "        # Restaurar capacidade original\n",
    "        self.planner.capacity_distribution = original_capacity\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
